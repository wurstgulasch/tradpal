{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59f68761",
   "metadata": {},
   "source": [
    "# TradPal - Machine Learning Training Guide\n",
    "\n",
    "This notebook demonstrates how to train machine learning models for signal enhancement in TradPal.\n",
    "\n",
    "## Overview\n",
    "- Load and prepare trading data\n",
    "- Train machine learning models\n",
    "- Evaluate model performance\n",
    "- Integrate models into trading signals\n",
    "\n",
    "## Requirements\n",
    "- TradPal with ML dependencies installed\n",
    "- Jupyter notebook environment\n",
    "- Sufficient computational resources for ML training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9afca09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add TradPal to path\n",
    "project_root = os.path.abspath('..')\n",
    "sys.path.append(project_root)\n",
    "\n",
    "# Import TradPal ML modules\n",
    "from src.ml_predictor import LSTMSignalPredictor, is_lstm_available, is_shap_available\n",
    "from src.ml_ensemble import EnsembleSignalPredictor\n",
    "from src.data_fetcher import fetch_historical_data\n",
    "from src.indicators import calculate_indicators\n",
    "from src.signal_generator import generate_signals\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"✅ Libraries imported successfully\")\n",
    "print(f\"📊 Working directory: {os.getcwd()}\")\n",
    "print(f\"🔧 LSTM available: {is_lstm_available()}\")\n",
    "print(f\"🔧 SHAP available: {is_shap_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebcb1f0",
   "metadata": {},
   "source": [
    "## Step 1: Configure ML Training Parameters\n",
    "\n",
    "Set up the parameters for machine learning model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f089690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML Training configuration\n",
    "ML_CONFIG = {\n",
    "    'symbol': 'BTC/USDT',\n",
    "    'exchange': 'kraken',\n",
    "    'timeframe': '1h',\n",
    "    'start_date': '2023-01-01',\n",
    "    'end_date': '2024-01-01',  # 1 year of training data\n",
    "    \n",
    "    # Data preparation\n",
    "    'test_size': 0.2,  # 20% for testing\n",
    "    'validation_size': 0.2,  # 20% of training data for validation\n",
    "    'random_state': 42,\n",
    "    \n",
    "    # Model parameters\n",
    "    'models_to_train': ['random_forest', 'xgboost', 'lstm'],\n",
    "    'cv_folds': 5,  # Cross-validation folds\n",
    "    \n",
    "    # Feature engineering\n",
    "    'lookback_periods': [5, 10, 20],  # Periods for feature creation\n",
    "    'include_price_features': True,\n",
    "    'include_volume_features': True,\n",
    "    'include_time_features': True,\n",
    "    \n",
    "    # Training parameters\n",
    "    'early_stopping_patience': 10,\n",
    "    'max_training_time': 300,  # Max training time in seconds\n",
    "}\n",
    "\n",
    "print(\"⚙️ ML Training configuration:\")\n",
    "for key, value in ML_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be845fbc",
   "metadata": {},
   "source": [
    "## Step 2: Prepare Training Data\n",
    "\n",
    "Fetch historical data and create features for machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9115d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"📥 Fetching training data for {ML_CONFIG['symbol']}...\")\n",
    "print(f\"   Period: {ML_CONFIG['start_date']} to {ML_CONFIG['end_date']}\")\n",
    "print(f\"   Timeframe: {ML_CONFIG['timeframe']}\")\n",
    "\n",
    "# Fetch historical data\n",
    "try:\n",
    "    raw_data = fetch_historical_data(\n",
    "        symbol=ML_CONFIG['symbol'],\n",
    "        exchange=ML_CONFIG['exchange'],\n",
    "        timeframe=ML_CONFIG['timeframe'],\n",
    "        start_date=ML_CONFIG['start_date'],\n",
    "        end_date=ML_CONFIG['end_date']\n",
    "    )\n",
    "    \n",
    "    print(f\"✅ Training data fetched: {len(raw_data)} candles\")\n",
    "    print(f\"   Date range: {raw_data.index[0]} to {raw_data.index[-1]}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error fetching training data: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f4610c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🔧 Preparing features and labels...\")\n",
    "\n",
    "# Calculate technical indicators\n",
    "indicator_config = {\n",
    "    'ema': {'enabled': True, 'periods': [9, 21, 50]},\n",
    "    'rsi': {'enabled': True, 'period': 14},\n",
    "    'bb': {'enabled': True, 'period': 20, 'std_dev': 2.0},\n",
    "    'atr': {'enabled': True, 'period': 14},\n",
    "    'adx': {'enabled': True, 'period': 14},\n",
    "    'macd': {'enabled': True}\n",
    "}\n",
    "\n",
    "data_with_indicators = calculate_indicators(raw_data, config=indicator_config)\n",
    "print(f\"✅ Indicators calculated: {len(data_with_indicators.columns)} features\")\n",
    "\n",
    "# Generate signals (these will be our labels)\n",
    "data_with_signals = generate_signals(data_with_indicators)\n",
    "print(\"✅ Signals generated for labels\")\n",
    "\n",
    "# Create target variable (1 for buy, -1 for sell, 0 for hold)\n",
    "data_with_signals['target'] = 0\n",
    "data_with_signals.loc[data_with_signals['Buy_Signal'] == 1, 'target'] = 1\n",
    "data_with_signals.loc[data_with_signals['Sell_Signal'] == 1, 'target'] = -1\n",
    "\n",
    "# Remove rows with NaN values\n",
    "data_clean = data_with_signals.dropna()\n",
    "print(f\"✅ Data cleaned: {len(data_clean)} samples\")\n",
    "\n",
    "# Show class distribution\n",
    "class_counts = data_clean['target'].value_counts().sort_index()\n",
    "print(f\"\\n📊 Class distribution:\")\n",
    "print(f\"   Hold (0): {class_counts.get(0, 0)} samples ({class_counts.get(0, 0)/len(data_clean)*100:.1f}%)\")\n",
    "print(f\"   Buy (1): {class_counts.get(1, 0)} samples ({class_counts.get(1, 0)/len(data_clean)*100:.1f}%)\")\n",
    "print(f\"   Sell (-1): {class_counts.get(-1, 0)} samples ({class_counts.get(-1, 0)/len(data_clean)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781b6361",
   "metadata": {},
   "source": [
    "## Step 3: Feature Engineering\n",
    "\n",
    "Create additional features for better model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729d804f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🔧 Creating additional features...\")\n",
    "\n",
    "# Create lagged features\n",
    "def create_lagged_features(df, columns, lags=[1, 2, 3]):\n",
    "    \"\"\"Create lagged versions of specified columns.\"\"\"\n",
    "    for col in columns:\n",
    "        for lag in lags:\n",
    "            df[f'{col}_lag_{lag}'] = df[col].shift(lag)\n",
    "    return df\n",
    "\n",
    "# Price-based features\n",
    "price_columns = ['close', 'high', 'low', 'volume']\n",
    "data_featured = create_lagged_features(data_clean.copy(), price_columns, lags=[1, 2, 5])\n",
    "\n",
    "# Technical indicator features\n",
    "indicator_columns = ['EMA9', 'EMA21', 'EMA50', 'RSI', 'BB_middle', 'BB_upper', 'BB_lower', 'ATR', 'ADX']\n",
    "data_featured = create_lagged_features(data_featured, indicator_columns, lags=[1, 2])\n",
    "\n",
    "# Create trend features\n",
    "data_featured['ema_trend'] = (data_featured['EMA9'] > data_featured['EMA21']).astype(int)\n",
    "data_featured['rsi_overbought'] = (data_featured['RSI'] > 70).astype(int)\n",
    "data_featured['rsi_oversold'] = (data_featured['RSI'] < 30).astype(int)\n",
    "data_featured['price_above_bb'] = (data_featured['close'] > data_featured['BB_upper']).astype(int)\n",
    "data_featured['price_below_bb'] = (data_featured['close'] < data_featured['BB_lower']).astype(int)\n",
    "\n",
    "# Time-based features\n",
    "data_featured['hour'] = data_featured.index.hour\n",
    "data_featured['day_of_week'] = data_featured.index.dayofweek\n",
    "data_featured['month'] = data_featured.index.month\n",
    "\n",
    "# Remove rows with NaN (from lagging)\n",
    "data_featured = data_featured.dropna()\n",
    "\n",
    "print(f\"✅ Feature engineering completed: {len(data_featured.columns)} total features\")\n",
    "print(f\"   Samples: {len(data_featured)}\")\n",
    "\n",
    "# Show feature categories\n",
    "feature_cols = [col for col in data_featured.columns if col not in ['open', 'high', 'low', 'close', 'volume', 'Buy_Signal', 'Sell_Signal', 'target']]\n",
    "print(f\"   Feature columns: {len(feature_cols)}\")\n",
    "print(f\"   Sample features: {feature_cols[:10]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5035dd1",
   "metadata": {},
   "source": [
    "## Step 4: Train Machine Learning Models\n",
    "\n",
    "Train and evaluate multiple ML models for signal prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fa38f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🤖 Training machine learning models...\")\n",
    "\n",
    "# Prepare features and target\n",
    "X = data_featured[feature_cols]\n",
    "y = data_featured['target']\n",
    "\n",
    "# Convert target to classification (0=hold, 1=buy, 2=sell)\n",
    "y_class = y.copy()\n",
    "y_class = y_class.replace({-1: 2})  # -1 (sell) -> 2\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_class, test_size=ML_CONFIG['test_size'], \n",
    "    random_state=ML_CONFIG['random_state'], stratify=y_class\n",
    ")\n",
    "\n",
    "print(f\"📊 Data split:\")\n",
    "print(f\"   Training: {len(X_train)} samples\")\n",
    "print(f\"   Testing: {len(X_test)} samples\")\n",
    "print(f\"   Features: {X.shape[1]}\")\n",
    "\n",
    "# Scale features for neural networks\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train models\n",
    "models = {}\n",
    "model_scores = {}\n",
    "\n",
    "# Random Forest\n",
    "if 'random_forest' in ML_CONFIG['models_to_train']:\n",
    "    print(\"\\n🌲 Training Random Forest...\")\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    \n",
    "    rf_model = RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=10,\n",
    "        random_state=ML_CONFIG['random_state'],\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    rf_scores = cross_val_score(rf_model, X_train, y_train, cv=ML_CONFIG['cv_folds'])\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    \n",
    "    models['random_forest'] = rf_model\n",
    "    model_scores['random_forest'] = {\n",
    "        'cv_mean': rf_scores.mean(),\n",
    "        'cv_std': rf_scores.std(),\n",
    "        'test_score': rf_model.score(X_test, y_test)\n",
    "    }\n",
    "    \n",
    "    print(f\"   CV Score: {rf_scores.mean():.3f} (+/- {rf_scores.std() * 2:.3f})\")\n",
    "    print(f\"   Test Score: {rf_model.score(X_test, y_test):.3f}\")\n",
    "\n",
    "# XGBoost\n",
    "if 'xgboost' in ML_CONFIG['models_to_train']:\n",
    "    print(\"\\n🚀 Training XGBoost...\")\n",
    "    try:\n",
    "        import xgboost as xgb\n",
    "        \n",
    "        xgb_model = xgb.XGBClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=6,\n",
    "            learning_rate=0.1,\n",
    "            random_state=ML_CONFIG['random_state'],\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        xgb_scores = cross_val_score(xgb_model, X_train, y_train, cv=ML_CONFIG['cv_folds'])\n",
    "        xgb_model.fit(X_train, y_train)\n",
    "        \n",
    "        models['xgboost'] = xgb_model\n",
    "        model_scores['xgboost'] = {\n",
    "            'cv_mean': xgb_scores.mean(),\n",
    "            'cv_std': xgb_scores.std(),\n",
    "            'test_score': xgb_model.score(X_test, y_test)\n",
    "        }\n",
    "        \n",
    "        print(f\"   CV Score: {xgb_scores.mean():.3f} (+/- {xgb_scores.std() * 2:.3f})\")\n",
    "        print(f\"   Test Score: {xgb_model.score(X_test, y_test):.3f}\")\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"   ⚠️ XGBoost not available, skipping\")\n",
    "\n",
    "# LSTM\n",
    "if 'lstm' in ML_CONFIG['models_to_train'] and is_lstm_available():\n",
    "    print(\"\\n🧠 Training LSTM...\")\n",
    "    \n",
    "    # Prepare sequential data for LSTM\n",
    "    sequence_length = 20\n",
    "    X_seq = []\n",
    "    y_seq = []\n",
    "    \n",
    "    for i in range(sequence_length, len(X_train_scaled)):\n",
    "        X_seq.append(X_train_scaled[i-sequence_length:i])\n",
    "        y_seq.append(y_train.iloc[i])\n",
    "    \n",
    "    X_seq = np.array(X_seq)\n",
    "    y_seq = np.array(y_seq)\n",
    "    \n",
    "    print(f\"   Sequential data: {X_seq.shape[0]} samples, {X_seq.shape[1]} timesteps, {X_seq.shape[2]} features\")\n",
    "    \n",
    "    # Use TradPal's LSTM predictor\n",
    "    lstm_predictor = LSTMSignalPredictor(\n",
    "        input_size=X_seq.shape[2],\n",
    "        hidden_size=64,\n",
    "        num_layers=2,\n",
    "        output_size=3  # 3 classes: hold, buy, sell\n",
    "    )\n",
    "    \n",
    "    # Train LSTM (simplified)\n",
    "    lstm_predictor.train(X_seq, y_seq, epochs=10, batch_size=32)\n",
    "    \n",
    "    models['lstm'] = lstm_predictor\n",
    "    model_scores['lstm'] = {\n",
    "        'cv_mean': 0.0,  # Would need proper CV implementation\n",
    "        'cv_std': 0.0,\n",
    "        'test_score': 0.0  # Would need evaluation\n",
    "    }\n",
    "    \n",
    "    print(\"   LSTM training completed (basic implementation)\")\n",
    "\n",
    "print(f\"\\n✅ Model training completed: {len(models)} models trained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b298482",
   "metadata": {},
   "source": [
    "## Step 5: Model Evaluation and Comparison\n",
    "\n",
    "Evaluate model performance and compare results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d90d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"📊 Evaluating model performance...\")\n",
    "\n",
    "# Create comparison dataframe\n",
    "results_df = pd.DataFrame.from_dict(model_scores, orient='index')\n",
    "results_df = results_df.round(4)\n",
    "\n",
    "print(\"\\n🏆 Model Performance Comparison:\")\n",
    "print(\"=\" * 50)\n",
    "print(results_df)\n",
    "\n",
    "# Visualize model comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# CV Scores\n",
    "axes[0].bar(results_df.index, results_df['cv_mean'], yerr=results_df['cv_std'], \n",
    "           capsize=5, color=['skyblue', 'lightgreen', 'salmon'])\n",
    "axes[0].set_title('Cross-Validation Scores')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].set_ylim(0, 1)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Test Scores\n",
    "axes[1].bar(results_df.index, results_df['test_score'], \n",
    "           color=['skyblue', 'lightgreen', 'salmon'])\n",
    "axes[1].set_title('Test Set Scores')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_ylim(0, 1)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Detailed evaluation for best model\n",
    "best_model_name = results_df['test_score'].idxmax()\n",
    "best_model = models[best_model_name]\n",
    "\n",
    "print(f\"\\n🎯 Best performing model: {best_model_name.upper()}\")\n",
    "print(f\"   Test Accuracy: {results_df.loc[best_model_name, 'test_score']:.3f}\")\n",
    "\n",
    "# Generate predictions for detailed evaluation\n",
    "if best_model_name != 'lstm':  # Skip LSTM for now\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    \n",
    "    print(\"\\n📋 Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=['Hold', 'Buy', 'Sell']))\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Hold', 'Buy', 'Sell'],\n",
    "                yticklabels=['Hold', 'Buy', 'Sell'])\n",
    "    plt.title(f'Confusion Matrix - {best_model_name.upper()}')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc1f30b",
   "metadata": {},
   "source": [
    "## Step 6: Feature Importance Analysis\n",
    "\n",
    "Analyze which features are most important for predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342868a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🔍 Analyzing feature importance...\")\n",
    "\n",
    "# Feature importance for tree-based models\n",
    "if 'random_forest' in models:\n",
    "    rf_model = models['random_forest']\n",
    "    \n",
    "    # Get feature importance\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'importance': rf_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    # Plot top 20 features\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    top_features = feature_importance.head(20)\n",
    "    sns.barplot(data=top_features, x='importance', y='feature')\n",
    "    plt.title('Top 20 Feature Importance - Random Forest')\n",
    "    plt.xlabel('Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n🔝 Top 10 Most Important Features:\")\n",
    "    for i, row in feature_importance.head(10).iterrows():\n",
    "        print(f\"   {row['feature']}: {row['importance']:.4f}\")\n",
    "\n",
    "if 'xgboost' in models:\n",
    "    xgb_model = models['xgboost']\n",
    "    \n",
    "    # Plot feature importance\n",
    "    try:\n",
    "        xgb.plot_importance(xgb_model, max_num_features=20)\n",
    "        plt.title('Feature Importance - XGBoost')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    except:\n",
    "        print(\"\\n⚠️ Could not plot XGBoost feature importance\")\n",
    "\n",
    "# SHAP analysis if available\n",
    "if is_shap_available() and 'random_forest' in models:\n",
    "    print(\"\\n🔮 Performing SHAP analysis...\")\n",
    "    \n",
    "    try:\n",
    "        import shap\n",
    "        \n",
    "        # Create explainer\n",
    "        explainer = shap.TreeExplainer(rf_model)\n",
    "        \n",
    "        # Calculate SHAP values for a sample\n",
    "        sample_data = X_test.head(100)  # Small sample for speed\n",
    "        shap_values = explainer.shap_values(sample_data)\n",
    "        \n",
    "        # Summary plot\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        shap.summary_plot(shap_values, sample_data, max_display=10, show=False)\n",
    "        plt.title('SHAP Feature Importance Summary')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"✅ SHAP analysis completed\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ SHAP analysis failed: {e}\")\n",
    "else:\n",
    "    print(\"\\n⚠️ SHAP not available or no suitable model for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63eef0ea",
   "metadata": {},
   "source": [
    "## Step 7: Save Trained Models\n",
    "\n",
    "Save the trained models for later use in signal prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d06078",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"💾 Saving trained models...\")\n",
    "\n",
    "import joblib\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Create models directory if it doesn't exist\n",
    "models_dir = os.path.join(project_root, 'cache', 'ml_models')\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "# Save models and metadata\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "saved_models = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    try:\n",
    "        if model_name == 'lstm':\n",
    "            # Special handling for LSTM (PyTorch model)\n",
    "            model_path = os.path.join(models_dir, f'{model_name}_model_{timestamp}.pth')\n",
    "            # Note: Would need proper PyTorch save logic here\n",
    "            print(f\"   ⚠️ LSTM model saving not implemented in this demo\")\n",
    "        else:\n",
    "            # Save sklearn models\n",
    "            model_path = os.path.join(models_dir, f'{model_name}_model_{timestamp}.pkl')\n",
    "            joblib.dump(model, model_path)\n",
    "            saved_models[model_name] = model_path\n",
    "            print(f\"   ✅ Saved {model_name} to {os.path.basename(model_path)}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Failed to save {model_name}: {e}\")\n",
    "\n",
    "# Save scaler\n",
    "scaler_path = os.path.join(models_dir, f'scaler_{timestamp}.pkl')\n",
    "joblib.dump(scaler, scaler_path)\n",
    "print(f\"   ✅ Saved feature scaler to {os.path.basename(scaler_path)}\")\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    'timestamp': timestamp,\n",
    "    'symbol': ML_CONFIG['symbol'],\n",
    "    'timeframe': ML_CONFIG['timeframe'],\n",
    "    'training_period': f\"{ML_CONFIG['start_date']} to {ML_CONFIG['end_date']}\",\n",
    "    'models_trained': list(models.keys()),\n",
    "    'best_model': best_model_name,\n",
    "    'feature_columns': feature_cols,\n",
    "    'model_scores': model_scores,\n",
    "    'scaler_path': scaler_path\n",
    "}\n",
    "\n",
    "metadata_path = os.path.join(models_dir, f'metadata_{timestamp}.json')\n",
    "import json\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2, default=str)\n",
    "\n",
    "print(f\"   ✅ Saved training metadata to {os.path.basename(metadata_path)}\")\n",
    "print(f\"\\n📁 Models saved to: {models_dir}\")\n",
    "print(f\"\\n💡 To use these models in TradPal, update the configuration in config/settings.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9682ebf",
   "metadata": {},
   "source": [
    "*TradPal v2.5.0 - Machine Learning for Educational Purposes Only*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
