import pytest
import pandas as pd
import numpy as np
import json
import os
import tempfile
from unittest.mock import patch, MagicMock, mock_open
from datetime import datetime, timedelta, timezone
import sys

# Add project paths for testing
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..', '..', 'services'))
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'config'))
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'integrations'))

from services.data_fetcher import fetch_historical_data, validate_data
from services.indicators import calculate_indicators, ema, rsi, bb, atr, adx
from services.signal_generator import generate_signals, calculate_risk_management
from services.output import save_signals_to_json, load_signals_from_json, format_signal_data
from services.backtester import run_backtest, calculate_performance_metrics, simulate_trades
from config.settings import validate_timeframe, get_timeframe_params, validate_risk_params
from integrations.base import IntegrationManager
from integrations.telegram.bot import TelegramIntegration, TelegramConfig
from integrations.email_integration.email import EmailIntegration, EmailConfig


class TestDataFetcherEdgeCases:
    """Test edge cases in data fetcher."""

    @patch('src.data_fetcher.cache_api_call')  # Disable caching for tests
    @patch('src.data_fetcher.get_data_source')
    def test_fetch_single_candle(self, mock_get_data_source, mock_cache):
        """Test fetching a single candle."""
        # Make cache decorator a no-op
        mock_cache.return_value = lambda func: func
        
        mock_data_source = MagicMock()
        mock_get_data_source.return_value = mock_data_source
        mock_data_source.fetch_historical_data.return_value = pd.DataFrame({
            'open': [1.0],
            'high': [1.1],
            'low': [0.9],
            'close': [1.05],
            'volume': [1000]
        }, index=pd.to_datetime([1640995200000], unit='ms'))

        result = fetch_historical_data('EUR/USD', '1m', 1)
        assert len(result) == 1
        assert result.iloc[0]['close'] == 1.05

    @patch('src.data_fetcher.cache_api_call')  # Disable caching for tests
    @patch('src.data_fetcher.get_data_source')
    def test_fetch_weekend_data(self, mock_get_data_source, mock_cache):
        """Test fetching data that includes weekends."""
        # Make cache decorator a no-op
        mock_cache.return_value = lambda func: func
        
        mock_data_source = MagicMock()
        mock_get_data_source.return_value = mock_data_source

        # Simulate weekend data (should be handled gracefully)
        weekend_data = pd.DataFrame({
            'open': [1.0, 1.05, 1.1],
            'high': [1.1, 1.15, 1.2],
            'low': [0.9, 0.95, 1.0],
            'close': [1.05, 1.1, 1.15],
            'volume': [1000, 800, 1200]
        }, index=pd.to_datetime([1640995200000, 1641081600000, 1641168000000], unit='ms'))
        mock_data_source.fetch_historical_data.return_value = weekend_data

        result = fetch_historical_data('EUR/USD', '1d', 3)
        assert len(result) >= 1  # At least some data should be returned

    @patch('src.data_fetcher.cache_api_call')  # Disable caching for tests
    @patch('src.data_fetcher.get_data_source')
    def test_fetch_with_timezone_issues(self, mock_get_data_source, mock_cache):
        """Test fetching data with timezone complications."""
        # Make cache decorator a no-op
        mock_cache.return_value = lambda func: func
        
        mock_data_source = MagicMock()
        mock_get_data_source.return_value = mock_data_source

        # Data with timestamps that might cause timezone issues
        data = pd.DataFrame({
            'open': [1.0],
            'high': [1.1],
            'low': [0.9],
            'close': [1.05],
            'volume': [1000]
        }, index=pd.to_datetime([1640995200000], unit='ms'))
        mock_data_source.fetch_historical_data.return_value = data

        result = fetch_historical_data('EUR/USD', '1m', 1)
        assert len(result) == 1
        # Should handle timezone conversion properly
        assert isinstance(result.index, pd.DatetimeIndex)

    def test_validate_data_with_minimum_required_data(self):
        """Test validation with minimum required data points."""
        # Minimum viable data for basic analysis
        df = pd.DataFrame({
            'timestamp': pd.date_range('2023-01-01', periods=2, freq='1min'),
            'open': [1.0, 1.01],
            'high': [1.02, 1.03],
            'low': [0.99, 1.0],
            'close': [1.01, 1.02],
            'volume': [100, 150]
        })

        # Should validate successfully
        validate_data(df)

    def test_validate_data_with_extreme_volume(self):
        """Test validation with extreme volume values."""
        df = pd.DataFrame({
            'timestamp': pd.date_range('2023-01-01', periods=5),
            'open': [1.0] * 5,
            'high': [1.1] * 5,
            'low': [0.9] * 5,
            'close': [1.05] * 5,
            'volume': [0, 1, 1e10, 1e15, 1e20]  # Extreme volumes
        })

        # Should still validate (extreme volumes are allowed)
        validate_data(df)


class TestIndicatorsEdgeCases:
    """Test edge cases in indicators."""

    def test_ema_with_period_larger_than_data(self):
        """Test EMA with period larger than available data."""
        data = pd.Series([1.0, 2.0, 3.0, 4.0, 5.0])
        result = ema(data, 10)  # Period > data length

        # Should return all NaN
        assert result.isna().all()

    def test_ema_with_period_equal_to_data(self):
        """Test EMA with period equal to data length."""
        data = pd.Series([1.0, 2.0, 3.0])
        result = ema(data, 3)

        # Should have some valid values
        assert not result.isna().all()
        # Last value should be a weighted average, not equal to the last data point
        assert result.iloc[-1] != data.iloc[-1]
        assert not pd.isna(result.iloc[-1])

    def test_rsi_with_minimum_data(self):
        """Test RSI with minimum required data."""
        data = pd.Series([100.0, 101.0, 102.0, 103.0, 104.0, 103.0, 102.0, 101.0, 102.0, 103.0, 104.0, 105.0, 104.0, 103.0, 102.0, 101.0, 100.0, 99.0, 98.0, 97.0])  # Variable data
        result = rsi(data, 14)

        # Should have at least one valid RSI value
        assert not result.isna().all()
        # RSI should be between 0 and 100 for valid values
        valid_rsi = result.dropna()
        if len(valid_rsi) > 0:
            assert all((valid_rsi >= 0) & (valid_rsi <= 100))

    def test_bb_with_minimum_data(self):
        """Test Bollinger Bands with minimum data."""
        data = pd.Series([100.0] * 20)  # Minimum for BB-20
        upper, middle, lower = bb(data, 20, 2)

        # Should have valid values at the end
        assert not upper.isna().all()
        assert not middle.isna().all()
        assert not lower.isna().all()

    def test_atr_with_price_gaps(self):
        """Test ATR with significant price gaps."""
        high = pd.Series([100.0, 110.0, 90.0, 120.0, 80.0])  # Large gaps
        low = pd.Series([95.0, 105.0, 85.0, 115.0, 75.0])
        close = pd.Series([98.0, 108.0, 88.0, 118.0, 78.0])

        result = atr(high, low, close, 3)

        # Should handle large gaps
        assert not result.isna().all()
        assert result.iloc[-1] > 0  # Should have positive ATR

    def test_adx_with_trending_data(self):
        """Test ADX with strongly trending data."""
        # Create strongly trending data
        base = 100.0
        high = pd.Series([base + i*2 for i in range(20)])  # Strong uptrend
        low = pd.Series([base + i*1.5 for i in range(20)])
        close = pd.Series([base + i*1.8 for i in range(20)])

        adx_values, di_plus, di_minus = adx(high, low, close, 14)

        # Should have high ADX values for trending data
        valid_adx = adx_values.dropna()
        if len(valid_adx) > 0:
            assert any(valid_adx > 25)  # Should detect trend

    def test_calculate_indicators_with_sparse_data(self):
        """Test indicator calculation with sparse/uneven data."""
        df = pd.DataFrame({
            'timestamp': pd.date_range('2023-01-01', periods=50, freq='1min'),
            'open': [100.0] * 50,
            'high': [101.0] * 50,
            'low': [99.0] * 50,
            'close': [100.5] * 50,
            'volume': [1000] * 50
        })

        result = calculate_indicators(df)

        # Should calculate all indicators
        expected_indicators = ['EMA9', 'EMA21', 'RSI', 'BB_upper', 'BB_middle', 'BB_lower', 'ATR']
        for indicator in expected_indicators:
            assert indicator in result.columns

    def test_indicators_with_precision_issues(self):
        """Test indicators with floating point precision issues."""
        # Use prices that might cause precision issues
        data = pd.Series([1.23456789] * 20)
        result = ema(data, 9)

        # Should handle precision gracefully
        assert not result.isna().all()
        # Check that results are reasonable
        assert result.dropna().between(1.2, 1.3).all()


class TestSignalGeneratorEdgeCases:
    """Test edge cases in signal generator."""

    def test_generate_signals_with_all_signals_true(self):
        """Test signal generation when all conditions are met."""
        from unittest.mock import patch, MagicMock
        
        # Create data with price information that will generate the expected indicators
        df = pd.DataFrame({
            'timestamp': pd.date_range('2023-01-01', periods=50),  # More data for indicator calculation
            'open': [100.0] * 50,
            'high': [101.0] * 50,
            'low': [99.0] * 50,
            'close': [100.5] * 50,
            'volume': [1000] * 50
        })

        # Calculate indicators first (required for signal generation)
        df = calculate_indicators(df)

        # Mock MTA to return the dataframe unchanged
        with patch('src.signal_generator.apply_multi_timeframe_analysis') as mock_mta:
            mock_mta.side_effect = lambda df: df  # Return input dataframe unchanged
            
            result = generate_signals(df)

            # Should have calculated indicators
            assert 'EMA9' in result.columns
            assert 'EMA21' in result.columns
            assert 'RSI' in result.columns
            assert 'BB_lower' in result.columns
            
            # Check if any buy signals were generated (should be true for this trending data)
            assert 'Buy_Signal' in result.columns
            assert 'Sell_Signal' in result.columns
            # With STRICT_SIGNALS_ENABLED=True, signals depend on RSI and BB conditions
            # For this flat data, RSI will be around 50, so may not generate signals
            # Just check that signal columns exist and are numeric
            assert result['Buy_Signal'].dtype in ['int64', 'int32', 'float64']
            assert result['Sell_Signal'].dtype in ['int64', 'int32', 'float64']

    def test_generate_signals_with_no_signals(self):
        """Test signal generation when no conditions are met."""
        df = pd.DataFrame({
            'timestamp': pd.date_range('2023-01-01', periods=50),  # More data for indicator calculation
            'open': [100.0] * 50,
            'high': [101.0] * 50,
            'low': [99.0] * 50,
            'close': [100.5] * 50,  # Flat prices
            'volume': [1000] * 50
        })

        # Calculate indicators first (required for signal generation)
        df = calculate_indicators(df)

        result = generate_signals(df)

        # Should have calculated indicators and signal columns
        assert 'Buy_Signal' in result.columns
        assert 'Sell_Signal' in result.columns
        assert 'EMA9' in result.columns
        assert 'EMA21' in result.columns
        assert 'RSI' in result.columns
        
        # For flat data, EMA crossover may not occur consistently
        # RSI will be neutral (around 50), so strict signals may not trigger
        # Just verify the columns exist and contain valid data
        assert len(result) == 50
        assert not result['Buy_Signal'].isna().all()
        assert not result['Sell_Signal'].isna().all()

    def test_risk_management_with_minimum_position_size(self):
        """Test risk management with very small position sizes."""
        df = pd.DataFrame({
            'timestamp': pd.date_range('2023-01-01', periods=10),
            'open': [100.0] * 10,
            'high': [101.0] * 10,
            'low': [99.0] * 10,
            'close': [100.5] * 10,
            'volume': [1000] * 10,
            'EMA9': [100.6] * 10,
            'EMA21': [100.4] * 10,
            'RSI': [25.0] * 10,
            'BB_upper': [101.5] * 10,
            'BB_middle': [100.5] * 10,
            'BB_lower': [99.5] * 10,
            'ATR': [0.01] * 10,  # Very small ATR
            'Buy_Signal': [1] * 10,
            'Sell_Signal': [0] * 10
        })

        result = calculate_risk_management(df)

        # Should still calculate position sizes
        assert 'Position_Size_Absolute' in result.columns
        assert (result['Position_Size_Absolute'] > 0).any()

    def test_risk_management_with_maximum_leverage(self):
        """Test risk management with maximum leverage scenarios."""
        df = pd.DataFrame({
            'timestamp': pd.date_range('2023-01-01', periods=10),
            'open': [100.0] * 10,
            'high': [101.0] * 10,
            'low': [99.0] * 10,
            'close': [100.5] * 10,
            'volume': [1000] * 10,
            'EMA9': [100.6] * 10,
            'EMA21': [100.4] * 10,
            'RSI': [25.0] * 10,
            'BB_upper': [101.5] * 10,
            'BB_middle': [100.5] * 10,
            'BB_lower': [99.5] * 10,
            'ATR': [10.0] * 10,  # Very high ATR (high volatility)
            'Buy_Signal': [1] * 10,
            'Sell_Signal': [0] * 10
        })

        result = calculate_risk_management(df)

        # Should limit leverage appropriately
        assert 'Leverage' in result.columns
        # Leverage should be reasonable (not extremely high)
        valid_leverage = result['Leverage'].dropna()
        if len(valid_leverage) > 0:
            assert all(valid_leverage <= 100)  # Reasonable upper limit


class TestOutputEdgeCases:
    """Test edge cases in output module."""

    def test_save_load_empty_signals(self):
        """Test saving and loading empty signal lists."""
        with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:
            temp_file = f.name

        try:
            # Save empty list
            save_signals_to_json([], temp_file)

            # Load empty list
            result = load_signals_from_json(temp_file)
            assert result == []

        finally:
            os.unlink(temp_file)

    def test_format_signal_data_with_special_characters(self):
        """Test formatting data with special characters."""
        data = pd.DataFrame({
            'timestamp': pd.to_datetime(['2023-01-01']),
            'open': [1.0],
            'high': [1.1],
            'low': [0.9],
            'close': [1.05],
            'volume': [1000],
            'EMA9': [1.02],
            'EMA21': [1.01],
            'RSI': [55.5],
            'BB_upper': [1.15],
            'BB_middle': [1.05],
            'BB_lower': [0.95],
            'ATR': [0.1],
            'Buy_Signal': [1],
            'Sell_Signal': [0],
            'Position_Size_Absolute': [1000.0],
            'Stop_Loss': [0.95],
            'Take_Profit': [1.15],
            'Leverage': [5.0]
        })

        result = format_signal_data(data)
        assert len(result) == 1

        # Should handle all data types properly
        signal = result[0]
        assert isinstance(signal['timestamp'], str)
        assert isinstance(signal['Buy_Signal'], bool)
        # Position_Size_Absolute should be in risk_management dict
        assert 'risk_management' in signal
        assert 'position_size' in signal['risk_management']

    def test_format_signal_data_with_nan_values(self):
        """Test formatting data with NaN values."""
        data = pd.DataFrame({
            'timestamp': pd.to_datetime(['2023-01-01']),
            'open': [1.0],
            'high': [1.1],
            'low': [0.9],
            'close': [1.05],
            'volume': [1000],
            'EMA9': [np.nan],  # NaN value
            'EMA21': [1.01],
            'RSI': [55.5],
            'BB_upper': [1.15],
            'BB_middle': [1.05],
            'BB_lower': [0.95],
            'ATR': [0.1],
            'Buy_Signal': [1],
            'Sell_Signal': [0]
        })

        result = format_signal_data(data)
        assert len(result) == 1

        # NaN should be converted to null
        assert result[0]['EMA9'] is None or str(result[0]['EMA9']) == 'nan'


class TestBacktesterEdgeCases:
    """Test edge cases in backtester."""

    def test_calculate_performance_with_single_trade(self):
        """Test performance calculation with a single trade."""
        trades = pd.DataFrame({
            'entry_price': [100.0],
            'exit_price': [110.0],
            'position_size': [1000],
            'direction': ['long'],
            'entry_time': pd.to_datetime(['2023-01-01']),
            'exit_time': pd.to_datetime(['2023-01-02'])
        })

        metrics = calculate_performance_metrics(trades, 10000)

        assert metrics['total_trades'] == 1
        assert metrics['winning_trades'] == 1
        assert metrics['losing_trades'] == 0
        assert metrics['win_rate'] == 100.0
        assert metrics['final_capital'] > 10000  # Should have profit

    def test_calculate_performance_with_all_losses(self):
        """Test performance calculation with all losing trades."""
        trades = pd.DataFrame({
            'entry_price': [100.0, 110.0],
            'exit_price': [95.0, 105.0],  # Both losses
            'position_size': [1000, 1000],
            'direction': ['long', 'long'],
            'entry_time': pd.to_datetime(['2023-01-01', '2023-01-02']),
            'exit_time': pd.to_datetime(['2023-01-03', '2023-01-04'])
        })

        metrics = calculate_performance_metrics(trades, 10000)

        assert metrics['total_trades'] == 2
        assert metrics['winning_trades'] == 0
        assert metrics['losing_trades'] == 2
        assert metrics['win_rate'] == 0.0
        assert metrics['final_capital'] < 10000  # Should have losses

    def test_simulate_trades_with_no_signals(self):
        """Test trade simulation with no trading signals."""
        data = pd.DataFrame({
            'timestamp': pd.date_range('2023-01-01', periods=10),
            'open': [100.0] * 10,
            'high': [101.0] * 10,
            'low': [99.0] * 10,
            'close': [100.5] * 10,
            'volume': [1000] * 10,
            'EMA9': [100.4] * 10,
            'EMA21': [100.6] * 10,
            'RSI': [50.0] * 10,
            'BB_upper': [101.5] * 10,
            'BB_middle': [100.5] * 10,
            'BB_lower': [99.5] * 10,
            'ATR': [0.5] * 10,
            'Buy_Signal': [0] * 10,  # No signals
            'Sell_Signal': [0] * 10
        })

        trades = simulate_trades(data)

        # Should return empty DataFrame
        assert len(trades) == 0

    def test_simulate_trades_with_continuous_signals(self):
        """Test trade simulation with continuous signals."""
        data = pd.DataFrame({
            'timestamp': pd.date_range('2023-01-01', periods=10),
            'open': [100.0] * 10,
            'high': [101.0] * 10,
            'low': [99.0] * 10,
            'close': [100.5] * 10,
            'volume': [1000] * 10,
            'EMA9': [100.6] * 10,
            'EMA21': [100.4] * 10,
            'RSI': [25.0] * 10,
            'BB_upper': [101.5] * 10,
            'BB_middle': [100.5] * 10,
            'BB_lower': [99.5] * 10,
            'ATR': [0.5] * 10,
            'Buy_Signal': [1] * 10,  # Continuous buy signals
            'Sell_Signal': [0] * 10
        })

        trades = simulate_trades(data)

        # Should handle continuous signals appropriately - may have incomplete trades
        assert isinstance(trades, pd.DataFrame)
        # For continuous signals, might have trades without exits
        if len(trades) > 0:
            assert 'entry_time' in trades.columns
            assert 'entry_price' in trades.columns

    @patch('src.backtester.fetch_historical_data')
    def test_run_backtest_with_minimum_date_range(self, mock_fetch):
        """Test backtest with minimum date range."""
        # Mock data for minimum range
        mock_data = pd.DataFrame({
            'timestamp': pd.date_range('2023-01-01', periods=50, freq='1min'),
            'open': [100.0] * 50,
            'high': [101.0] * 50,
            'low': [99.0] * 50,
            'close': [100.5] * 50,
            'volume': [1000] * 50
        })
        mock_fetch.return_value = mock_data

        result = run_backtest('EUR/USD', '1m', '2023-01-01', '2023-01-02')

        # Should complete successfully - may have no trades if no signals generated
        assert isinstance(result, dict)
        # Either has metrics/trades or an error message about no trades
        assert ('backtest_results' in result and 'trades' in result) or 'error' in result['backtest_results']


class TestConfigurationEdgeCases:
    """Test edge cases in configuration."""

    def test_timeframe_params_for_extreme_timeframes(self):
        """Test timeframe parameters for extreme cases."""
        # Very short timeframe (existing)
        params_1m = get_timeframe_params('1m')
        assert params_1m is not None

        # Very long timeframe (existing)
        params_1d = get_timeframe_params('1d')
        assert params_1d is not None

    def test_risk_params_boundary_values(self):
        """Test risk parameters at boundary values."""
        # Minimum valid values
        assert validate_risk_params({
            'capital': 1,  # Minimum capital
            'risk_per_trade': 0.001,  # 0.1%
            'sl_multiplier': 0.1  # Minimum stop loss
        })

        # Maximum reasonable values
        assert validate_risk_params({
            'capital': 10000000,  # Large capital
            'risk_per_trade': 0.1,  # 10%
            'sl_multiplier': 5.0  # Large stop loss
        })

    def test_timeframe_validation_boundary_cases(self):
        """Test timeframe validation at boundaries."""
        # Valid edge cases
        assert validate_timeframe('1s')  # Minimum
        assert validate_timeframe('1M')  # Maximum

        # Invalid edge cases
        assert not validate_timeframe('0s')  # Zero not allowed
        assert not validate_timeframe('1x')  # Invalid unit


class TestIntegrationEdgeCases:
    """Test edge cases in integrations."""

    def test_telegram_with_empty_message(self):
        """Test Telegram integration with empty messages."""
        config = TelegramConfig(enabled=True, name="Test", bot_token="token", chat_id="123")
        integration = TelegramIntegration(config)

        result = integration.send_signal({})  # Empty signal
        # Should handle gracefully (may succeed or fail based on implementation)

    def test_email_with_special_characters(self):
        """Test Email integration with special characters."""
        config = EmailConfig(
            enabled=True,
            name="Test",
            smtp_server="smtp.example.com",
            username="tëst@ëxämple.com",  # Special chars in email
            password="pâssword"
        )
        integration = EmailIntegration(config)

        # Should handle special characters in configuration
        assert integration.config.username == "tëst@ëxämple.com"

    def test_integration_manager_with_many_integrations(self):
        """Test integration manager with many integrations."""
        manager = IntegrationManager()

        # Register many integrations
        for i in range(10):
            config = TelegramConfig(
                enabled=True,
                name=f"Test{i}",
                bot_token=f"token{i}",
                chat_id=f"chat{i}"
            )
            integration = TelegramIntegration(config)
            manager.register_integration(f"test{i}", integration)

        # Should handle many integrations
        assert len(manager.integrations) == 10

        # Initialize all
        results = manager.initialize_all()
        assert len(results) == 10


class TestSystemIntegrationEdgeCases:
    """Test edge cases across the entire system."""

    def test_full_pipeline_with_minimum_data(self):
        """Test full pipeline with minimum viable data."""
        # Create minimum data that can flow through the entire pipeline
        df = pd.DataFrame({
            'timestamp': pd.date_range('2023-01-01', periods=50, freq='1min'),
            'open': [100.0] * 50,
            'high': [101.0] * 50,
            'low': [99.0] * 50,
            'close': [100.5] * 50,
            'volume': [1000] * 50
        })

        # Run through full pipeline
        df_with_indicators = calculate_indicators(df)
        df_with_signals = generate_signals(df_with_indicators)
        df_with_risk = calculate_risk_management(df_with_signals)

        # Should complete without errors
        assert len(df_with_risk) == 50
        assert 'Buy_Signal' in df_with_risk.columns
        assert 'Position_Size_Absolute' in df_with_risk.columns

    def test_pipeline_with_varying_volatility(self):
        """Test pipeline with varying volatility levels."""
        # Create data with increasing volatility
        timestamps = pd.date_range('2023-01-01', periods=100, freq='1min')
        base_price = 100.0

        # Increasing volatility
        volatility_factors = np.linspace(0.001, 0.1, 100)

        close_prices = []
        high_prices = []
        low_prices = []

        for i, vol in enumerate(volatility_factors):
            price = base_price + np.sin(i * 0.1) * 10
            close_prices.append(price)
            high_prices.append(price * (1 + vol))
            low_prices.append(price * (1 - vol))

        df = pd.DataFrame({
            'timestamp': timestamps,
            'open': close_prices,
            'high': high_prices,
            'low': low_prices,
            'close': close_prices,
            'volume': [1000 + i*10 for i in range(100)]
        })

        # Run through pipeline
        df_with_indicators = calculate_indicators(df)
        df_with_signals = generate_signals(df_with_indicators)
        df_with_risk = calculate_risk_management(df_with_signals)

        # Should handle varying volatility
        assert len(df_with_risk) == 100
        assert not df_with_risk['ATR'].isna().all()  # Should have ATR values

    def test_memory_efficiency_with_large_datasets(self):
        """Test memory efficiency with large datasets."""
        # Create large dataset
        n_rows = 10000
        df = pd.DataFrame({
            'timestamp': pd.date_range('2020-01-01', periods=n_rows, freq='1min'),
            'open': np.random.randn(n_rows) + 100,
            'high': np.random.randn(n_rows) + 101,
            'low': np.random.randn(n_rows) + 99,
            'close': np.random.randn(n_rows) + 100.5,
            'volume': np.random.randint(1000, 10000, n_rows)
        })

        try:
            # Process large dataset
            df_with_indicators = calculate_indicators(df)
            df_with_signals = generate_signals(df_with_indicators)
            df_with_risk = calculate_risk_management(df_with_signals)

            # Should complete successfully
            assert len(df_with_risk) == n_rows

        except MemoryError:
            # Acceptable for very large datasets
            pytest.skip("Memory limit reached for large dataset test")


if __name__ == "__main__":
    pytest.main([__file__])