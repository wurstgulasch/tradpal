# TradPal Indicator - Test Suite

## ðŸ“ Test Structure

Die Test-Suite spiegelt die Projektstruktur wider und ist wie folgt organisiert:

```
tests/
â”œâ”€â”€ config/                    # Tests fÃ¼r Konfiguration
â”‚   â””â”€â”€ test_config.py
â”œâ”€â”€ src/                       # Tests fÃ¼r Kernmodule
â”‚   â”œâ”€â”€ test_data_fetcher.py
â”‚   â”œâ”€â”€ test_indicators.py
â”‚   â”œâ”€â”€ test_output.py
â”‚   â””â”€â”€ test_backtester.py
â”œâ”€â”€ integrations/              # Tests fÃ¼r Integrationen
â”‚   â””â”€â”€ test_integrations.py
â”œâ”€â”€ scripts/                   # Tests fÃ¼r Skripte
â”‚   â””â”€â”€ test_scripts.py
â”œâ”€â”€ test_error_handling.py     # Umfassende Fehlerbehandlungstests
â”œâ”€â”€ test_edge_cases.py         # GrenzfÃ¤lle und spezielle Szenarien
â”œâ”€â”€ test_performance.py        # Performance- und Lasttests
â””â”€â”€ __init__.py
```

## ðŸ§ª Test-Kategorien

### Unit Tests
- **data_fetcher**: Datenabruf und Validierung von Exchanges
- **indicators**: Technische Indikatoren (EMA, RSI, BB, ATR, ADX)
- **output**: JSON-Formatierung und Dateioperationen
- **backtester**: Historische Backtest-FunktionalitÃ¤t
- **config**: Konfigurationsparameter und Validierung

### Integration Tests
- **integrations**: Telegram/Email-Benachrichtigungen
- **scripts**: CLI-Tools und Automatisierungsskripte

### Spezielle Tests
- **error_handling**: Umfassende Fehlerbehandlung fÃ¼r alle Module
- **edge_cases**: GrenzfÃ¤lle und ungewÃ¶hnliche Szenarien
- **performance**: Performance-Benchmarks und Lasttests

## ðŸš€ Tests AusfÃ¼hren

### Alle Tests
```bash
python run_tests.py
```

### Mit ausfÃ¼hrlicher Ausgabe
```bash
python run_tests.py --verbose
```

### Mit Coverage-Report
```bash
python run_tests.py --coverage
```

### Spezifische Testdateien
```bash
python run_tests.py --test-files tests/src/test_data_fetcher.py tests/src/test_indicators.py
```

### Einzelne Test-Module
```bash
# Nur Data Fetcher Tests
python -m pytest tests/src/test_data_fetcher.py -v

# Nur Performance Tests
python -m pytest tests/test_performance.py -v
```

## ðŸ“Š Test-Abdeckung

Die Test-Suite bietet umfassende Abdeckung fÃ¼r:

- âœ… **120+ individuelle Tests**
- âœ… Unit Tests fÃ¼r alle Kernfunktionen
- âœ… Integration Tests fÃ¼r externe Services
- âœ… Error Handling fÃ¼r alle Fehlerszenarien
- âœ… Edge Cases fÃ¼r GrenzfÃ¤lle
- âœ… Performance Benchmarks
- âœ… Mock-basierte Tests fÃ¼r APIs

## ðŸ› ï¸ Test-Framework

- **pytest**: Moderne Test-Framework mit umfangreichen Features
- **unittest.mock**: Mocking fÃ¼r externe APIs und Services
- **pandas/numpy**: Datenverarbeitung in Tests
- **tempfile**: TemporÃ¤re Dateien fÃ¼r I/O-Tests

## ðŸ“ˆ QualitÃ¤tsmetriken

- **Zero Failures**: Alle Tests mÃ¼ssen bestehen
- **High Coverage**: >90% Code-Abdeckung angestrebt
- **Fast Execution**: Tests laufen in <30 Sekunden
- **Reliable**: Deterministische, nicht-flaky Tests

## ðŸ”§ Test-Entwicklung

### Neue Tests hinzufÃ¼gen:
1. Testdatei im entsprechenden Unterordner erstellen
2. pytest-Konventionen folgen (test_*.py)
3. Umfassende Docstrings und Kommentare
4. Mock externe Dependencies

### Test-Namenskonventionen:
- `test_function_name()`: Unit Tests
- `test_feature_scenario()`: Integration Tests
- `TestClassName`: Test-Klassen
- `test_method_name()`: Methoden in Test-Klassen

## ðŸ“‹ CI/CD Integration

Die Tests sind fÃ¼r automatische AusfÃ¼hrung in CI/CD-Pipelines optimiert:

- Parallele AusfÃ¼hrung mÃ¶glich
- JUnit/XML Output fÃ¼r Berichterstattung
- Coverage-Reports fÃ¼r QualitÃ¤tsmetriken
- Fail-Fast bei kritischen Fehlern

---

*Automatisch generiert fÃ¼r TradPal Indicator v2.0*