#!/usr/bin/env python3
"""
ML Trainer Service - Machine learning model training and optimization.

Provides comprehensive ML training capabilities including:
- Model training with various algorithms
- Hyperparameter optimization with Optuna
- Model evaluation and validation
- Feature engineering and selection
- Model persistence and management
"""

import asyncio
import logging
import os
import pickle
import json
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, Any, Optional, List, Tuple
from dataclasses import dataclass, asdict

import numpy as np
import pandas as pd
import optuna
from sklearn.ensemble import StackingClassifier, VotingClassifier, RandomForestClassifier, GradientBoostingClassifier, BaggingClassifier, AdaBoostClassifier
from sklearn.linear_model import LogisticRegression
from scipy.optimize import minimize
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from sklearn.model_selection import TimeSeriesSplit
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import SelectKBest, f_classif
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
import warnings
warnings.filterwarnings('ignore', category=UserWarning)

from config.settings import (
    ML_MODELS_DIR,
    ML_FEATURES,
    ML_TARGET_HORIZON,
    ML_TRAINING_WINDOW,
    ML_VALIDATION_SPLIT,
    ML_RANDOM_STATE
)

# Local imports
from .shap_interpreter import SHAPInterpreter
from services.data_service.data_service.alternative_data.client import AlternativeDataService

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class ModelMetadata:
    """Metadata for trained ML models."""
    name: str
    symbol: str
    timeframe: str
    model_type: str
    created_at: datetime
    training_start: str
    training_end: str
    target_horizon: int
    features: List[str]
    hyperparameters: Dict[str, Any]
    performance: Dict[str, float]
    feature_importance: Dict[str, float]


@dataclass
class TrainingStatus:
    """Status of model training."""
    symbol: str
    status: str  # 'idle', 'training', 'completed', 'failed'
    progress: float
    message: str
    start_time: Optional[datetime] = None
    end_time: Optional[datetime] = None


class EventSystem:
    """Simple event system for service communication."""

    def __init__(self):
        self._handlers: Dict[str, List[callable]] = {}

    def subscribe(self, event_type: str, handler: callable):
        """Subscribe to an event type."""
        if event_type not in self._handlers:
            self._handlers[event_type] = []
        self._handlers[event_type].append(handler)

    async def publish(self, event_type: str, data: Dict[str, Any]):
        """Publish an event."""
        if event_type in self._handlers:
            for handler in self._handlers[event_type]:
                try:
                    if asyncio.iscoroutinefunction(handler):
                        await handler(data)
                    else:
                        handler(data)
                except Exception as e:
                    logger.error(f"Event handler error: {e}")


class MLTrainerService:
    """ML model training and optimization service."""

    def __init__(self, event_system: Optional[EventSystem] = None):
        self.event_system = event_system or EventSystem()
        self.models_dir = Path(ML_MODELS_DIR)
        self.models_dir.mkdir(exist_ok=True)

        # Training status tracking
        self.training_status: Dict[str, TrainingStatus] = {}

        # Model cache
        self._model_cache: Dict[str, Any] = {}

        # Initialize scalers and selectors
        self.scalers: Dict[str, StandardScaler] = {}
        self.selectors: Dict[str, SelectKBest] = {}

        # Initialize ensemble trainer
        self.ensemble_trainer = EnsembleTrainer()

        # Initialize market regime detector
        self.regime_detector = MarketRegimeDetector()

        # Initialize SHAP interpreter
        self.shap_interpreter = SHAPInterpreter()

        # Initialize Alternative Data Service
        self.alternative_data_service = AlternativeDataService()

        logger.info("ML Trainer Service initialized")

    async def health_check(self) -> Dict[str, Any]:
        """Perform health check."""
        return {
            "service": "ml_trainer",
            "status": "healthy",
            "timestamp": datetime.now().isoformat(),
            "models_count": len(await self.list_models()),
            "active_training": len([s for s in self.training_status.values() if s.status == "training"])
        }

    async def train_model(
        self,
        symbol: str,
        timeframe: str,
        start_date: str,
        end_date: str,
        model_type: str = "random_forest",
        target_horizon: int = ML_TARGET_HORIZON,
        use_optuna: bool = False
    ) -> Dict[str, Any]:
        """
        Train an ML model for trading signals.

        Args:
            symbol: Trading symbol
            timeframe: Timeframe
            start_date: Training start date
            end_date: Training end date
            model_type: Type of ML model
            target_horizon: Prediction horizon
            use_optuna: Whether to use Optuna for optimization

        Returns:
            Training results
        """
        model_name = f"{symbol}_{timeframe}_{model_type}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

        # Update training status
        self.training_status[symbol] = TrainingStatus(
            symbol=symbol,
            status="training",
            progress=0.0,
            message="Starting training...",
            start_time=datetime.now()
        )

        try:
            # Fetch training data (placeholder - would integrate with data service)
            logger.info(f"Fetching training data for {symbol} {timeframe}")
            training_data = await self._fetch_training_data(symbol, timeframe, start_date, end_date)

            if training_data.empty:
                raise ValueError("No training data available")

            self.training_status[symbol].progress = 0.2
            self.training_status[symbol].message = "Data loaded, preparing features..."

            # Prepare features and target
            X, y, feature_names = self._prepare_features(training_data, target_horizon)

            if len(X) == 0:
                raise ValueError("Insufficient data for training")

            self.training_status[symbol].progress = 0.4
            self.training_status[symbol].message = "Features prepared, training model..."

            # Train model
            if use_optuna:
                model, best_params = await self._train_with_optuna(X, y, model_type)
            else:
                model = self._train_model(X, y, model_type)
                best_params = self._get_default_params(model_type)

            self.training_status[symbol].progress = 0.7
            self.training_status[symbol].message = "Model trained, evaluating..."

            # Evaluate model
            performance = self._evaluate_model(model, X, y)

            # Get feature importance
            feature_importance = self._get_feature_importance(model, feature_names)

            self.training_status[symbol].progress = 0.9
            self.training_status[symbol].message = "Saving model..."

        # Save model and metadata
            metadata = ModelMetadata(
                name=model_name,
                symbol=symbol,
                timeframe=timeframe,
                model_type=model_type,
                created_at=datetime.now(),
                training_start=start_date,
                training_end=end_date,
                target_horizon=target_horizon,
                features=feature_names,
                hyperparameters=best_params,
                performance=performance,
                feature_importance=feature_importance
            )

            await self._save_model(model_name, model, metadata)

            # Generate SHAP interpretability report
            try:
                logger.info("Generating SHAP interpretability report...")
                shap_report = self.shap_interpreter.generate_interpretability_report(
                    model_name, X, y
                )
                await self._save_shap_report(model_name, shap_report)
                logger.info("SHAP interpretability report generated")
            except Exception as e:
                logger.warning(f"Failed to generate SHAP report: {e}")

            # Update status
            self.training_status[symbol].status = "completed"
            self.training_status[symbol].progress = 1.0
            self.training_status[symbol].message = "Training completed"
            self.training_status[symbol].end_time = datetime.now()

            # Publish event
            await self.event_system.publish("ml.training_completed", {
                "model_name": model_name,
                "symbol": symbol,
                "performance": performance
            })

            logger.info(f"Model training completed: {model_name}")

            return {
                "success": True,
                "model_name": model_name,
                "performance": performance,
                "feature_importance": feature_importance,
                "shap_available": True
            }

        except Exception as e:
            # Update status on failure
            self.training_status[symbol].status = "failed"
            self.training_status[symbol].message = str(e)
            self.training_status[symbol].end_time = datetime.now()

            logger.error(f"Model training failed: {e}")
            raise

    async def retrain_model(
        self,
        model_name: str,
        new_data_start: str,
        new_data_end: str
    ) -> Dict[str, Any]:
        """
        Retrain an existing model with new data.

        Args:
            model_name: Name of the model to retrain
            new_data_start: Start date for new data
            new_data_end: End date for new data

        Returns:
            Retraining results
        """
        # Load existing model and metadata
        model, metadata = await self._load_model(model_name)

        # Fetch new data
        new_data = await self._fetch_training_data(
            metadata.symbol,
            metadata.timeframe,
            new_data_start,
            new_data_end
        )

        # Combine with existing training data
        combined_data = await self._fetch_training_data(
            metadata.symbol,
            metadata.timeframe,
            metadata.training_start,
            new_data_end
        )

        # Prepare features
        X, y, _ = self._prepare_features(combined_data, metadata.target_horizon)

        # Retrain model
        model = self._train_model(X, y, metadata.model_type)

        # Evaluate
        performance = self._evaluate_model(model, X, y)

        # Update metadata
        metadata.performance = performance
        metadata.training_end = new_data_end

        # Save updated model
        await self._save_model(model_name, model, metadata)

        return {
            "success": True,
            "model_name": model_name,
            "performance": performance
        }

    async def evaluate_model(
        self,
        model_name: str,
        test_data: pd.DataFrame
    ) -> Dict[str, float]:
        """
        Evaluate a trained model on test data.

        Args:
            model_name: Name of the model to evaluate
            test_data: Test data

        Returns:
            Performance metrics
        """
        model, metadata = await self._load_model(model_name)

        # Prepare test features
        X_test, y_test, _ = self._prepare_features(test_data, metadata.target_horizon)

        # Evaluate
        performance = self._evaluate_model(model, X_test, y_test)

        # Publish event
        await self.event_system.publish("ml.model_evaluated", {
            "model_name": model_name,
            "performance": performance
        })

        return performance

    async def list_models(self) -> List[str]:
        """List all trained models."""
        model_files = list(self.models_dir.glob("*.pkl"))
        return [f.stem for f in model_files]

    async def get_model_info(self, model_name: str) -> Dict[str, Any]:
        """Get information about a specific model."""
        _, metadata = await self._load_model(model_name)
        return asdict(metadata)

    async def delete_model(self, model_name: str) -> bool:
        """Delete a trained model."""
        model_path = self.models_dir / f"{model_name}.pkl"
        metadata_path = self.models_dir / f"{model_name}_metadata.pkl"

        success = True
        if model_path.exists():
            model_path.unlink()
        else:
            success = False

        if metadata_path.exists():
            metadata_path.unlink()

        # Clear from cache
        if model_name in self._model_cache:
            del self._model_cache[model_name]

        return success

    async def get_feature_importance(self, symbol: str, timeframe: str) -> Dict[str, float]:
        """Get feature importance for recent models."""
        models = await self.list_models()
        symbol_models = [m for m in models if m.startswith(f"{symbol}_{timeframe}")]

        if not symbol_models:
            return {}

        # Get latest model
        latest_model = max(symbol_models)
        _, metadata = await self._load_model(latest_model)

        return metadata.feature_importance

    def get_hyperparameter_ranges(self, model_type: str) -> Dict[str, Any]:
        """Get hyperparameter ranges for a model type."""
        ranges = {
            "random_forest": {
                "n_estimators": {"type": "int", "low": 50, "high": 500},
                "max_depth": {"type": "int", "low": 5, "high": 50},
                "min_samples_split": {"type": "int", "low": 2, "high": 20},
                "min_samples_leaf": {"type": "int", "low": 1, "high": 10}
            },
            "gradient_boosting": {
                "n_estimators": {"type": "int", "low": 50, "high": 300},
                "learning_rate": {"type": "float", "low": 0.01, "high": 0.3},
                "max_depth": {"type": "int", "low": 3, "high": 10},
                "subsample": {"type": "float", "low": 0.5, "high": 1.0}
            },
            "logistic_regression": {
                "C": {"type": "float", "low": 0.01, "high": 10.0},
                "penalty": {"type": "categorical", "choices": ["l1", "l2", "elasticnet"]}
            },
            "svm": {
                "C": {"type": "float", "low": 0.1, "high": 100.0},
                "gamma": {"type": "float", "low": 0.001, "high": 1.0},
                "kernel": {"type": "categorical", "choices": ["rbf", "linear", "poly"]}
            }
        }

        return ranges.get(model_type, {})

    async def get_training_status(self, symbol: str) -> Dict[str, Any]:
        """Get training status for a symbol."""
        status = self.training_status.get(symbol)
        if status:
            return asdict(status)
        return {"status": "idle", "message": "No training in progress"}

    async def _fetch_training_data(
        self,
        symbol: str,
        timeframe: str,
        start_date: str,
        end_date: str
    ) -> pd.DataFrame:
        """Fetch training data (placeholder - integrate with data service)."""
        # This would normally call the data service
        # For now, return sample data structure
        logger.warning("Using placeholder training data - integrate with data service")

        # Generate sample OHLCV data
        dates = pd.date_range(start=start_date, end=end_date, freq='1H')
        np.random.seed(42)

        data = {
            'timestamp': dates,
            'open': 50000 + np.random.normal(0, 1000, len(dates)),
            'high': 50000 + np.random.normal(0, 1000, len(dates)) + 100,
            'low': 50000 + np.random.normal(0, 1000, len(dates)) - 100,
            'close': 50000 + np.random.normal(0, 1000, len(dates)),
            'volume': np.random.normal(100, 20, len(dates))
        }

        df = pd.DataFrame(data)
        df['close'] = df['close'].clip(lower=0)  # Ensure positive prices

        return df

    def _prepare_features(self, data: pd.DataFrame, target_horizon: int) -> Tuple[np.ndarray, np.ndarray, List[str]]:
        """Prepare features and target for ML training."""
        # Calculate technical indicators
        df = data.copy()

        # Simple moving averages
        df['sma_20'] = df['close'].rolling(20).mean()
        df['sma_50'] = df['close'].rolling(50).mean()

        # RSI
        delta = df['close'].diff()
        gain = (delta.where(delta > 0, 0)).rolling(14).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(14).mean()
        rs = gain / loss
        df['rsi'] = 100 - (100 / (1 + rs))

        # MACD
        ema_12 = df['close'].ewm(span=12).mean()
        ema_26 = df['close'].ewm(span=26).mean()
        df['macd'] = ema_12 - ema_26
        df['macd_signal'] = df['macd'].ewm(span=9).mean()

        # Bollinger Bands
        sma_20 = df['close'].rolling(20).mean()
        std_20 = df['close'].rolling(20).std()
        df['bb_upper'] = sma_20 + (std_20 * 2)
        df['bb_lower'] = sma_20 - (std_20 * 2)
        df['bb_width'] = (df['bb_upper'] - df['bb_lower']) / sma_20

        # ATR
        high_low = df['high'] - df['low']
        high_close = (df['high'] - df['close'].shift(1)).abs()
        low_close = (df['low'] - df['close'].shift(1)).abs()
        tr = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)
        df['atr'] = tr.rolling(14).mean()

        # Volume indicators
        df['volume_sma'] = df['volume'].rolling(20).mean()
        df['volume_ratio'] = df['volume'] / df['volume_sma']

        # Price momentum
        df['returns'] = df['close'].pct_change()
        df['momentum'] = df['close'] / df['close'].shift(10) - 1

        # Target: future price movement
        df['future_return'] = df['close'].shift(-target_horizon) / df['close'] - 1
        df['target'] = (df['future_return'] > 0).astype(int)

        # Select features
        feature_cols = [
            'sma_20', 'sma_50', 'rsi', 'macd', 'macd_signal',
            'bb_upper', 'bb_lower', 'bb_width', 'atr',
            'volume_ratio', 'returns', 'momentum'
        ]

        # Drop NaN values
        df_clean = df.dropna(subset=feature_cols + ['target'])

        X = df_clean[feature_cols].values
        y = df_clean['target'].values

        return X, y, feature_cols

    def _prepare_enhanced_features(self, data: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray, List[str]]:
        """Prepare enhanced features for ML training with advanced indicators."""
        if data.empty:
            return np.array([]), np.array([]), []

        # Ensure we have the required columns
        required_cols = ['close', 'high', 'low', 'volume']
        if not all(col in data.columns for col in required_cols):
            raise ValueError(f"Missing required columns: {required_cols}")

        # Create target variable (price movement prediction)
        data = data.copy()
        data['target'] = (data['close'].shift(-ML_TARGET_HORIZON) > data['close']).astype(int)

        # Drop rows with NaN target
        data = data.dropna(subset=['target'])

        if len(data) < 50:  # Minimum required for feature engineering
            return np.array([]), np.array([]), []

        # Select features for training (exclude non-numeric and target columns)
        exclude_cols = ['target', 'timestamp', 'date', 'trend_strength', 'trend_direction',
                       'volatility_regime', 'momentum_regime'] + [col for col in data.columns if col.startswith('target_')]

        feature_cols = [col for col in data.columns if col not in exclude_cols and
                       data[col].dtype in ['int64', 'float64', 'int32', 'float32']]

        # Ensure we have features
        if not feature_cols:
            # Fallback to basic features
            feature_cols = ['close', 'volume'] if 'volume' in data.columns else ['close']

        # Prepare X and y
        X = data[feature_cols].fillna(0).values
        y = data['target'].values

        return X, y, feature_cols

    def _train_model(self, X: np.ndarray, y: np.ndarray, model_type: str) -> Any:
        """Train a model with default parameters."""
        if model_type == "random_forest":
            model = RandomForestClassifier(
                n_estimators=100,
                max_depth=10,
                random_state=ML_RANDOM_STATE
            )
        elif model_type == "gradient_boosting":
            model = GradientBoostingClassifier(
                n_estimators=100,
                learning_rate=0.1,
                max_depth=5,
                random_state=ML_RANDOM_STATE
            )
        elif model_type == "logistic_regression":
            model = LogisticRegression(random_state=ML_RANDOM_STATE)
        elif model_type == "svm":
            model = SVC(probability=True, random_state=ML_RANDOM_STATE)
        elif model_type == "xgboost":
            model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=ML_RANDOM_STATE)
        elif model_type == "lightgbm":
            model = LGBMClassifier(random_state=ML_RANDOM_STATE)
        else:
            raise ValueError(f"Unknown model type: {model_type}")

        model.fit(X, y)
        return model

    def _get_default_params(self, model_type: str) -> Dict[str, Any]:
        """Get default hyperparameters for a model type."""
        defaults = {
            "random_forest": {
                "n_estimators": 100,
                "max_depth": 10,
                "min_samples_split": 2,
                "min_samples_leaf": 1
            },
            "gradient_boosting": {
                "n_estimators": 100,
                "learning_rate": 0.1,
                "max_depth": 5,
                "subsample": 1.0
            },
            "logistic_regression": {
                "C": 1.0,
                "penalty": "l2"
            },
            "svm": {
                "C": 1.0,
                "gamma": "scale",
                "kernel": "rbf"
            },
            "xgboost": {
                "n_estimators": 100,
                "max_depth": 3,
                "learning_rate": 0.1
            },
            "lightgbm": {
                "n_estimators": 100,
                "max_depth": 3,
                "learning_rate": 0.1
            }
        }

        return defaults.get(model_type, {})

    async def _train_with_optuna(
        self,
        X: np.ndarray,
        y: np.ndarray,
        model_type: str
    ) -> Tuple[Any, Dict[str, Any]]:
        """Train a model with Optuna hyperparameter optimization."""

        def objective(trial):
            if model_type == "random_forest":
                params = {
                    "n_estimators": trial.suggest_int("n_estimators", 50, 500),
                    "max_depth": trial.suggest_int("max_depth", 5, 50),
                    "min_samples_split": trial.suggest_int("min_samples_split", 2, 20),
                    "min_samples_leaf": trial.suggest_int("min_samples_leaf", 1, 10)
                }
                model = RandomForestClassifier(**params, random_state=ML_RANDOM_STATE)

            elif model_type == "gradient_boosting":
                params = {
                    "n_estimators": trial.suggest_int("n_estimators", 50, 300),
                    "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3),
                    "max_depth": trial.suggest_int("max_depth", 3, 10),
                    "subsample": trial.suggest_float("subsample", 0.5, 1.0)
                }
                model = GradientBoostingClassifier(**params, random_state=ML_RANDOM_STATE)

            elif model_type == "xgboost":
                params = {
                    "n_estimators": trial.suggest_int("n_estimators", 50, 300),
                    "max_depth": trial.suggest_int("max_depth", 3, 10),
                    "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3)
                }
                model = XGBClassifier(**params, use_label_encoder=False, eval_metric='logloss', random_state=ML_RANDOM_STATE)

            elif model_type == "lightgbm":
                params = {
                    "n_estimators": trial.suggest_int("n_estimators", 50, 300),
                    "max_depth": trial.suggest_int("max_depth", 3, 10),
                    "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3)
                }
                model = LGBMClassifier(**params, random_state=ML_RANDOM_STATE)

            else:
                # For other models, use default training
                model = self._train_model(X, y, model_type)
                return 0.5  # Dummy score

            # Cross-validation
            tscv = TimeSeriesSplit(n_splits=3)
            scores = []

            for train_idx, val_idx in tscv.split(X):
                X_train, X_val = X[train_idx], X[val_idx]
                y_train, y_val = y[train_idx], y[val_idx]

                model.fit(X_train, y_train)
                y_pred = model.predict(X_val)
                scores.append(f1_score(y_val, y_pred))

            return np.mean(scores)

        # Run optimization
        study = optuna.create_study(direction="maximize")
        study.optimize(objective, n_trials=50)

        # Train final model with best parameters
        best_params = study.best_params
        model = self._train_model(X, y, model_type)

        # Apply best parameters if applicable
        if hasattr(model, 'set_params'):
            model.set_params(**best_params)

        model.fit(X, y)

        return model, best_params

    def _evaluate_model(self, model: Any, X: np.ndarray, y: np.ndarray) -> Dict[str, float]:
        """Evaluate model performance."""
        y_pred = model.predict(X)

        # Get prediction probabilities if available
        y_prob = None
        if hasattr(model, 'predict_proba'):
            y_prob = model.predict_proba(X)[:, 1]

        metrics = {
            "accuracy": accuracy_score(y, y_pred),
            "precision": precision_score(y, y_pred, zero_division=0),
            "recall": recall_score(y, y_pred, zero_division=0),
            "f1_score": f1_score(y, y_pred, zero_division=0)
        }

        # Only calculate ROC AUC if we have both classes present
        if y_prob is not None and len(np.unique(y)) > 1:
            try:
                metrics["roc_auc"] = roc_auc_score(y, y_prob)
            except ValueError:
                # ROC AUC not defined for single class
                metrics["roc_auc"] = 0.5

        return metrics

    def _get_feature_importance(self, model: Any, feature_names: List[str]) -> Dict[str, float]:
        """Get feature importance from model."""
        importance_dict = {}

        if hasattr(model, 'feature_importances_'):
            importances = model.feature_importances_
            for name, importance in zip(feature_names, importances):
                importance_dict[name] = float(importance)
        else:
            # For models without feature_importances_, assign equal weights
            for name in feature_names:
                importance_dict[name] = 1.0 / len(feature_names)

        return importance_dict

    async def _save_model(self, model_name: str, model: Any, metadata: ModelMetadata):
        """Save model and metadata to disk."""
        model_path = self.models_dir / f"{model_name}.pkl"
        metadata_path = self.models_dir / f"{model_name}_metadata.pkl"

        # Ensure directory exists
        model_path.parent.mkdir(parents=True, exist_ok=True)

        with open(model_path, 'wb') as f:
            pickle.dump(model, f)

        with open(metadata_path, 'wb') as f:
            pickle.dump(metadata, f)

        # Cache the model
        self._model_cache[model_name] = (model, metadata)

    async def _load_model(self, model_name: str) -> Tuple[Any, ModelMetadata]:
        """Load model and metadata from disk or cache."""
        if model_name in self._model_cache:
            return self._model_cache[model_name]

        model_path = self.models_dir / f"{model_name}.pkl"
        metadata_path = self.models_dir / f"{model_name}_metadata.pkl"

        if not model_path.exists() or not metadata_path.exists():
            raise FileNotFoundError(f"Model {model_name} not found")

        with open(model_path, 'rb') as f:
            model = pickle.load(f)

        with open(metadata_path, 'rb') as f:
            metadata = pickle.load(f)

        # Cache the model
        self._model_cache[model_name] = (model, metadata)

        return model, metadata

    async def train_benchmark_outperforming_model(
        self,
        symbol: str,
        timeframe: str,
        start_date: str,
        end_date: str,
        benchmark_metrics: Optional[Dict[str, float]] = None
    ) -> Dict[str, Any]:
        """
        Train ML models specifically designed to outperform benchmarks.

        Args:
            symbol: Trading symbol
            timeframe: Timeframe
            start_date: Training start date
            end_date: Training end date
            benchmark_metrics: Benchmark metrics to outperform (optional)

        Returns:
            Training results with outperformance analysis
        """
        model_name = f"{symbol}_{timeframe}_outperformance_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

        # Update training status
        self.training_status[symbol] = TrainingStatus(
            symbol=symbol,
            status="training",
            progress=0.0,
            message="Starting benchmark-outperforming ML training...",
            start_time=datetime.now()
        )

        try:
            # Fetch enhanced training data
            logger.info(f"Fetching enhanced training data for {symbol} {timeframe}")
            training_data = await self._fetch_enhanced_training_data(symbol, timeframe, start_date, end_date)

            if training_data.empty:
                raise ValueError("No training data available")

            self.training_status[symbol].progress = 0.2
            self.training_status[symbol].message = "Enhanced data loaded, engineering advanced features..."

            # Prepare enhanced features for outperformance
            X, y, feature_names = self._prepare_enhanced_features(training_data)

            if len(X) == 0:
                raise ValueError("Insufficient data for training")

            self.training_status[symbol].progress = 0.4
            self.training_status[symbol].message = "Advanced features prepared, training ensemble models..."

            # Train ensemble models for outperformance
            ensemble_result = self.ensemble_trainer.train_ensemble(X, y)

            self.training_status[symbol].progress = 0.8
            self.training_status[symbol].message = "Ensemble trained, evaluating outperformance..."

            # Evaluate outperformance against benchmarks
            outperformance_analysis = self._analyze_outperformance(
                ensemble_result, benchmark_metrics, X, y
            )

            self.training_status[symbol].progress = 0.9
            self.training_status[symbol].message = "Saving outperformance model..."

            # Save model and metadata
            metadata = ModelMetadata(
                name=model_name,
                symbol=symbol,
                timeframe=timeframe,
                model_type="ensemble_outperformance",
                created_at=datetime.now(),
                training_start=start_date,
                training_end=end_date,
                target_horizon=ML_TARGET_HORIZON,
                features=feature_names,
                hyperparameters={"ensemble": True, "benchmark_targeting": True},
                performance=ensemble_result['performance'],
                feature_importance=self.ensemble_trainer.feature_importance_ensemble
            )

            await self._save_model(model_name, ensemble_result['model'], metadata)

            # Update status
            self.training_status[symbol].status = "completed"
            self.training_status[symbol].progress = 1.0
            self.training_status[symbol].message = "Benchmark-outperforming model training completed"
            self.training_status[symbol].end_time = datetime.now()

            # Publish event
            await self.event_system.publish("ml.outperformance_training_completed", {
                "model_name": model_name,
                "symbol": symbol,
                "performance": ensemble_result['performance'],
                "outperformance": outperformance_analysis
            })

            logger.info(f"Benchmark-outperforming model training completed: {model_name}")

            return {
                "success": True,
                "model_name": model_name,
                "performance": ensemble_result['performance'],
                "outperformance_analysis": outperformance_analysis,
                "selected_model": ensemble_result['model_name'],
                "feature_importance": self.ensemble_trainer.feature_importance_ensemble
            }

        except Exception as e:
            # Update status on failure
            self.training_status[symbol].status = "failed"
            self.training_status[symbol].message = str(e)
            self.training_status[symbol].end_time = datetime.now()

            logger.error(f"Benchmark-outperforming training failed: {e}")
            raise

    def _analyze_outperformance(
        self,
        ensemble_result: Dict[str, Any],
        benchmark_metrics: Optional[Dict[str, float]],
        X: np.ndarray,
        y: np.ndarray
    ) -> Dict[str, Any]:
        """Analyze how well the model outperforms benchmarks."""
        model_performance = ensemble_result['performance']

        # Default benchmark metrics (typical ML model performance)
        default_benchmarks = {
            'accuracy': 0.55,  # Typical accuracy for price prediction
            'precision': 0.52,
            'recall': 0.50,
            'f1_score': 0.51,
            'roc_auc': 0.53
        }

        benchmarks = benchmark_metrics or default_benchmarks

        outperformance = {}
        for metric in ['accuracy', 'precision', 'recall', 'f1_score', 'roc_auc']:
            if metric in model_performance and metric in benchmarks:
                model_score = model_performance[metric]
                benchmark_score = benchmarks[metric]
                improvement = model_score - benchmark_score
                improvement_pct = (improvement / benchmark_score) * 100 if benchmark_score > 0 else 0

                outperformance[metric] = {
                    'model_score': model_score,
                    'benchmark_score': benchmark_score,
                    'improvement': improvement,
                    'improvement_pct': improvement_pct,
                    'outperforms': improvement > 0
                }

        # Calculate overall outperformance score
        outperforming_metrics = sum(1 for m in outperformance.values() if m['outperforms'])
        total_metrics = len(outperformance)
        overall_outperformance_pct = (outperforming_metrics / total_metrics) * 100

        # Trading-specific metrics
        if 'f1_score' in outperformance:
            f1_improvement = outperformance['f1_score']['improvement_pct']
            # Estimate potential profit improvement
            estimated_profit_boost = f1_improvement * 0.8  # Rough estimate

            outperformance['trading_impact'] = {
                'estimated_profit_boost_pct': estimated_profit_boost,
                'signal_quality_improvement': f1_improvement,
                'benchmark_outperformance_confidence': 'high' if overall_outperformance_pct > 70 else 'medium'
            }

        return {
            'metric_outperformance': outperformance,
            'overall_outperformance_pct': overall_outperformance_pct,
            'outperforming_metrics': outperforming_metrics,
            'total_metrics': total_metrics,
            'benchmark_comparison': benchmarks,
            'model_selected': ensemble_result['model_name']
        }

    def _prepare_enhanced_features(self, data: pd.DataFrame, is_validation: bool = False) -> Tuple[np.ndarray, np.ndarray, List[str]]:
        """Prepare enhanced features for ML training with advanced indicators."""
        if data.empty:
            return np.array([]), np.array([]), []

        # Ensure we have the required columns
        required_cols = ['close', 'high', 'low', 'volume']
        if not all(col in data.columns for col in required_cols):
            raise ValueError(f"Missing required columns: {required_cols}")

        # Create target variable (price movement prediction)
        data = data.copy()
        data['target'] = (data['close'].shift(-ML_TARGET_HORIZON) > data['close']).astype(int)

        # Drop rows with NaN target
        data = data.dropna(subset=['target'])

        # Adjust minimum requirements based on context
        min_samples = 10 if is_validation else 50  # Less strict for validation data

        if len(data) < min_samples:
            return np.array([]), np.array([]), []

        # Always use full feature engineering, but handle NaN values more robustly for validation
        data = self._add_advanced_technical_indicators(data)

        # For validation data, be more aggressive with NaN filling
        if is_validation:
            # Fill NaN values with forward/backward fill, then zeros
            data = data.ffill().bfill().fillna(0)
        else:
            # For training data, use standard NaN handling
            data = data.fillna(0)

        # Select features for training (exclude non-numeric and target columns)
        exclude_cols = ['target', 'timestamp', 'date', 'trend_strength', 'trend_direction',
                       'volatility_regime', 'momentum_regime'] + [col for col in data.columns if col.startswith('target_')]

        feature_cols = [col for col in data.columns if col not in exclude_cols and
                       data[col].dtype in ['int64', 'float64', 'int32', 'float32']]

        # Ensure we have features
        if not feature_cols:
            # Fallback to basic features
            feature_cols = ['close', 'volume'] if 'volume' in data.columns else ['close']

        # Prepare X and y
        X = data[feature_cols].fillna(0).values
        y = data['target'].values

        return X, y, feature_cols

    def _add_advanced_technical_indicators(self, df: pd.DataFrame) -> pd.DataFrame:
        """Add advanced technical indicators for enhanced feature engineering."""
        df = df.copy()

        # Basic price features
        df['returns'] = df['close'].pct_change()
        df['log_returns'] = np.log(df['close'] / df['close'].shift(1))

        # Moving averages
        for period in [5, 10, 20, 50]:
            df[f'sma_{period}'] = df['close'].rolling(window=period).mean()
            df[f'ema_{period}'] = df['close'].ewm(span=period).mean()

        # Volatility indicators
        df['volatility_20'] = df['returns'].rolling(window=20).std()
        df['volatility_50'] = df['returns'].rolling(window=50).std()

        # RSI
        delta = df['close'].diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
        rs = gain / loss
        df['rsi'] = 100 - (100 / (1 + rs))

        # MACD
        ema_12 = df['close'].ewm(span=12).mean()
        ema_26 = df['close'].ewm(span=26).mean()
        df['macd'] = ema_12 - ema_26
        df['macd_signal'] = df['macd'].ewm(span=9).mean()
        df['macd_hist'] = df['macd'] - df['macd_signal']

        # Bollinger Bands
        sma_20 = df['close'].rolling(window=20).mean()
        std_20 = df['close'].rolling(window=20).std()
        df['bb_upper'] = sma_20 + (std_20 * 2)
        df['bb_lower'] = sma_20 - (std_20 * 2)
        df['bb_middle'] = sma_20
        df['bb_width'] = (df['bb_upper'] - df['bb_lower']) / df['bb_middle']

        # Stochastic Oscillator
        df['stoch_k'], df['stoch_d'] = self._calculate_stochastic(df)

        # Williams %R
        df['williams_r'] = self._calculate_williams_r(df)

        # CCI (Commodity Channel Index)
        df['cci'] = self._calculate_cci(df)

        # ADX (Average Directional Index)
        df['adx'] = self._calculate_adx(df)

        # Volume indicators (if volume exists)
        if 'volume' in df.columns:
            df['volume_sma_20'] = df['volume'].rolling(window=20).mean()
            df['volume_ratio'] = df['volume'] / df['volume_sma_20']

            # OBV (On Balance Volume)
            df['obv'] = (df['volume'] * (df['close'].diff() > 0).astype(int) -
                        df['volume'] * (df['close'].diff() < 0).astype(int)).cumsum()

            # CMF (Chaikin Money Flow)
            df['cmf'] = self._calculate_chaikin_money_flow(df)

        # Momentum indicators
        for period in [5, 10, 20]:
            df[f'momentum_{period}'] = df['close'] / df['close'].shift(period) - 1

        # Rate of Change
        df['roc_10'] = (df['close'] - df['close'].shift(10)) / df['close'].shift(10)

        return df.fillna(0)

    def _calculate_stochastic(self, df: pd.DataFrame, k_period: int = 14, d_period: int = 3) -> Tuple[pd.Series, pd.Series]:
        """Calculate Stochastic Oscillator."""
        low_min = df['low'].rolling(window=k_period).min()
        high_max = df['high'].rolling(window=k_period).max()

        k = 100 * ((df['close'] - low_min) / (high_max - low_min))
        d = k.rolling(window=d_period).mean()

        return k, d

    def _calculate_williams_r(self, df: pd.DataFrame, period: int = 14) -> pd.Series:
        """Calculate Williams %R."""
        highest_high = df['high'].rolling(window=period).max()
        lowest_low = df['low'].rolling(window=period).min()

        williams_r = -100 * ((highest_high - df['close']) / (highest_high - lowest_low))
        return williams_r

    def _calculate_cci(self, df: pd.DataFrame, period: int = 20) -> pd.Series:
        """Calculate Commodity Channel Index."""
        typical_price = (df['high'] + df['low'] + df['close']) / 3
        sma_tp = typical_price.rolling(window=period).mean()
        mad_tp = typical_price.rolling(window=period).apply(lambda x: np.mean(np.abs(x - x.mean())))

        cci = (typical_price - sma_tp) / (0.015 * mad_tp)
        return cci

    def _calculate_adx(self, df: pd.DataFrame, period: int = 14) -> pd.Series:
        """Calculate Average Directional Index."""
        high_diff = df['high'] - df['high'].shift(1)
        low_diff = df['low'].shift(1) - df['low']

        plus_dm = np.where((high_diff > low_diff) & (high_diff > 0), high_diff, 0)
        minus_dm = np.where((low_diff > high_diff) & (low_diff > 0), low_diff, 0)

        tr = np.maximum(
            df['high'] - df['low'],
            np.maximum(
                abs(df['high'] - df['close'].shift(1)),
                abs(df['low'] - df['close'].shift(1))
            )
        )

        atr = pd.Series(tr).rolling(window=period).mean()

        plus_di = 100 * (pd.Series(plus_dm).rolling(window=period).mean() / atr)
        minus_di = 100 * (pd.Series(minus_dm).rolling(window=period).mean() / atr)

        dx = 100 * abs(plus_di - minus_di) / (plus_di + minus_di)
        adx = dx.rolling(window=period).mean()

        return adx

    def _calculate_chaikin_money_flow(self, df: pd.DataFrame, period: int = 21) -> pd.Series:
        """Calculate Chaikin Money Flow."""
        if 'volume' not in df.columns:
            return pd.Series([0] * len(df), index=df.index)

        # Money Flow Multiplier
        mfm = ((df['close'] - df['low']) - (df['high'] - df['close'])) / (df['high'] - df['low'])
        mfm = mfm.fillna(0)  # Handle division by zero

        # Money Flow Volume
        mfv = mfm * df['volume']

        # Chaikin Money Flow
        cmf = mfv.rolling(window=period).sum() / df['volume'].rolling(window=period).sum()
        return cmf

    def _add_market_regime_indicators(self, df: pd.DataFrame) -> pd.DataFrame:
        """Add advanced market regime detection indicators using ML."""
        try:
            # Use the advanced market regime detector
            return self.regime_detector.detect_market_regimes(df)
        except Exception as e:
            logger.warning(f"Advanced regime detection failed, falling back to basic: {e}")
            # Fallback to basic regime detection
            return self._add_basic_regime_indicators(df)
        """Add advanced market regime detection indicators using ML."""
        try:
            # Use the advanced market regime detector
            return self.regime_detector.detect_market_regimes(df)
        except Exception as e:
            logger.warning(f"Advanced regime detection failed, falling back to basic: {e}")
            # Fallback to basic regime detection
            return self._add_basic_regime_indicators(df)

    def _add_momentum_volatility_indicators(self, df: pd.DataFrame) -> pd.DataFrame:
        """Add momentum and volatility indicators."""
        df = df.copy()

        # Additional momentum indicators
        df['momentum_1d'] = df['close'] / df['close'].shift(1) - 1
        df['momentum_5d'] = df['close'] / df['close'].shift(5) - 1
        df['momentum_20d'] = df['close'] / df['close'].shift(20) - 1

        # Acceleration (rate of change of momentum)
        df['acceleration_5d'] = df['momentum_5d'] - df['momentum_5d'].shift(5)

        # Volatility measures
        df['close_volatility_5'] = df['close'].rolling(window=5).std()
        df['close_volatility_20'] = df['close'].rolling(window=20).std()
        df['returns_volatility_20'] = df['returns'].rolling(window=20).std()

        # Sharpe-like ratio (returns / volatility)
        df['sharpe_ratio_20'] = df['returns'].rolling(window=20).mean() / df['returns_volatility_20']

        # Volume volatility if volume exists
        if 'volume' in df.columns:
            df['volume_volatility_20'] = df['volume'].rolling(window=20).std()

        return df.fillna(0)
        """Add momentum and volatility indicators."""
        df = df.copy()

        # Additional momentum indicators
        df['momentum_1d'] = df['close'] / df['close'].shift(1) - 1
        df['momentum_5d'] = df['close'] / df['close'].shift(5) - 1
        df['momentum_20d'] = df['close'] / df['close'].shift(20) - 1

        # Acceleration (rate of change of momentum)
        df['acceleration_5d'] = df['momentum_5d'] - df['momentum_5d'].shift(5)

        # Volatility measures
        df['close_volatility_5'] = df['close'].rolling(window=5).std()
        df['close_volatility_20'] = df['close'].rolling(window=20).std()
        df['returns_volatility_20'] = df['returns'].rolling(window=20).std()

        # Sharpe-like ratio (returns / volatility)
        df['sharpe_ratio_20'] = df['returns'].rolling(window=20).mean() / df['returns_volatility_20']

        # Volume volatility if volume exists
        if 'volume' in df.columns:
            df['volume_volatility_20'] = df['volume'].rolling(window=20).std()

        return df.fillna(0)


        # Add more sophisticated technical indicators for outperformance
        df = self._add_advanced_technical_indicators(df)

        # Add market regime indicators
        df = self._add_market_regime_indicators(df)

        # Add momentum and volatility indicators
        df = self._add_momentum_volatility_indicators(df)

        # Integrate alternative data sources
        df = await self._integrate_alternative_data(df, symbol)

        return df

    async def perform_walk_forward_validation(
        self,
        symbol: str,
        timeframe: str,
        start_date: str,
        end_date: str,
        config: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Perform walk-forward validation for ML model evaluation.

        This method implements a realistic backtesting approach where:
        1. Models are trained on historical data (in-sample)
        2. Performance is evaluated on future unseen data (out-of-sample)
        3. This prevents overfitting and provides realistic performance estimates

        Args:
            symbol: Trading symbol
            timeframe: Timeframe
            start_date: Start date for validation period
            end_date: End date for validation period
            config: Validation configuration

        Returns:
            Walk-forward validation results
        """
        config = config or self._get_default_walk_forward_config()

        # Extract configuration
        initial_train_window = config.get('initial_train_window', 252)  # ~1 year
        validation_window = config.get('validation_window', 21)        # ~1 month
        step_size = config.get('step_size', validation_window)         # Monthly steps
        min_train_samples = config.get('min_train_samples', 100)
        model_type = config.get('model_type', 'ensemble_outperformance')

        logger.info(f"Starting walk-forward validation for {symbol} {timeframe}")
        logger.info(f"Config: Train={initial_train_window}, Val={validation_window}, Step={step_size}")

        # Update training status
        self.training_status[symbol] = TrainingStatus(
            symbol=symbol,
            status="training",
            progress=0.0,
            message="Starting walk-forward validation...",
            start_time=datetime.now()
        )

        try:
            # Fetch complete dataset
            logger.info("Fetching complete dataset for walk-forward validation...")
            full_data = await self._fetch_enhanced_training_data(symbol, timeframe, start_date, end_date)

            if full_data.empty or len(full_data) < initial_train_window + validation_window:
                raise ValueError("Insufficient data for walk-forward validation")

            self.training_status[symbol].progress = 0.1
            self.training_status[symbol].message = "Dataset loaded, preparing walk-forward windows..."

            # Generate walk-forward windows
            windows = self._generate_walk_forward_windows(
                full_data, initial_train_window, validation_window, step_size, min_train_samples
            )

            if not windows:
                raise ValueError("No valid walk-forward windows could be generated")

            logger.info(f"Generated {len(windows)} walk-forward windows")

            # Perform walk-forward validation
            validation_results = []
            total_windows = len(windows)

            for i, window in enumerate(windows):
                window_number = i + 1
                progress = 0.1 + (0.8 * (i / total_windows))

                self.training_status[symbol].progress = progress
                self.training_status[symbol].message = f"Processing window {window_number}/{total_windows}..."

                logger.info(f"Processing window {window_number}: IS={len(window['train_data'])}, OOS={len(window['validation_data'])}")

                # Train model on in-sample data and validate on out-of-sample data
                window_result = await self._train_and_validate_window(
                    window, window_number, model_type, config
                )

                validation_results.append(window_result)

            self.training_status[symbol].progress = 0.9
            self.training_status[symbol].message = "Walk-forward validation completed, analyzing results..."

            # Analyze overall results
            analysis = self._analyze_walk_forward_results(validation_results, config)

            # Generate comprehensive report
            report = self._generate_walk_forward_report(validation_results, analysis, config)

            # Update status
            self.training_status[symbol].status = "completed"
            self.training_status[symbol].progress = 1.0
            self.training_status[symbol].message = "Walk-forward validation completed"
            self.training_status[symbol].end_time = datetime.now()

            # Publish event
            await self.event_system.publish("ml.walk_forward_validation_completed", {
                "symbol": symbol,
                "timeframe": timeframe,
                "total_windows": len(validation_results),
                "analysis": analysis,
                "report": report
            })

            logger.info(f"Walk-forward validation completed: {len(validation_results)} windows processed")

            return {
                "success": True,
                "symbol": symbol,
                "timeframe": timeframe,
                "total_windows": len(validation_results),
                "validation_results": validation_results,
                "analysis": analysis,
                "report": report,
                "config": config
            }

        except Exception as e:
            # Update status on failure
            self.training_status[symbol].status = "failed"
            self.training_status[symbol].message = str(e)
            self.training_status[symbol].end_time = datetime.now()

            logger.error(f"Walk-forward validation failed: {e}")
            raise

    def _get_default_walk_forward_config(self) -> Dict[str, Any]:
        """Get default walk-forward validation configuration."""
        return {
            'initial_train_window': 252,  # ~1 year of daily data
            'validation_window': 21,      # ~1 month
            'step_size': 21,              # Monthly steps
            'min_train_samples': 100,
            'model_type': 'ensemble_outperformance',
            'benchmark_metrics': {
                'accuracy': 0.55,
                'precision': 0.52,
                'recall': 0.50,
                'f1_score': 0.51,
                'roc_auc': 0.53
            },
            'stability_threshold': 0.7,
            'min_windows': 6
        }

    def _generate_walk_forward_windows(
        self,
        data: pd.DataFrame,
        initial_train_window: int,
        validation_window: int,
        step_size: int,
        min_train_samples: int
    ) -> List[Dict[str, Any]]:
        """
        Generate walk-forward validation windows.

        Uses expanding window approach where training set grows over time.
        """
        data_length = len(data)

        # Check if we have sufficient data
        if data_length < initial_train_window + validation_window:
            raise ValueError(f"Insufficient data for walk-forward validation. "
                           f"Need at least {initial_train_window + validation_window} samples, "
                           f"but only have {data_length}.")

        windows = []
        current_train_end = initial_train_window

        while current_train_end + validation_window <= data_length:
            # Training data (expanding window)
            train_start_idx = 0
            train_end_idx = current_train_end

            # Validation data (future unseen data)
            val_start_idx = current_train_end
            val_end_idx = min(current_train_end + validation_window, data_length)

            # Extract windows
            train_data = data.iloc[train_start_idx:train_end_idx].copy()
            validation_data = data.iloc[val_start_idx:val_end_idx].copy()

            # Ensure sufficient training samples
            if len(train_data) >= min_train_samples:
                window = {
                    'window_number': len(windows) + 1,
                    'train_start_idx': train_start_idx,
                    'train_end_idx': train_end_idx,
                    'validation_start_idx': val_start_idx,
                    'validation_end_idx': val_end_idx,
                    'train_data': train_data,
                    'validation_data': validation_data,
                    'train_period': {
                        'start': data.index[train_start_idx].isoformat() if hasattr(data.index[train_start_idx], 'isoformat') else str(data.index[train_start_idx]),
                        'end': data.index[train_end_idx-1].isoformat() if hasattr(data.index[train_end_idx-1], 'isoformat') else str(data.index[train_end_idx-1])
                    },
                    'validation_period': {
                        'start': data.index[val_start_idx].isoformat() if hasattr(data.index[val_start_idx], 'isoformat') else str(data.index[val_start_idx]),
                        'end': data.index[val_end_idx-1].isoformat() if hasattr(data.index[val_end_idx-1], 'isoformat') else str(data.index[val_end_idx-1])
                    }
                }
                windows.append(window)

            # Move to next window
            current_train_end += step_size

        return windows

    async def _train_and_validate_window(
        self,
        window: Dict[str, Any],
        window_number: int,
        model_type: str,
        config: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Train model on in-sample data and validate on out-of-sample data."""
        try:
            # Prepare training features
            X_train, y_train, feature_names = self._prepare_enhanced_features(window['train_data'])

            if len(X_train) == 0:
                raise ValueError(f"Insufficient training data in window {window_number}")

            # Train model
            if model_type == 'ensemble_outperformance':
                # Use ensemble trainer for outperformance
                ensemble_result = self.ensemble_trainer.train_ensemble(X_train, y_train)
                model = ensemble_result['model']
                in_sample_performance = ensemble_result['performance']
            else:
                # Use standard model training
                model = self._train_model(X_train, y_train, model_type)
                in_sample_performance = self._evaluate_model(model, X_train, y_train)

            # Prepare validation features
            X_val, y_val, _ = self._prepare_enhanced_features(window['validation_data'], is_validation=True)

            if len(X_val) == 0 or len(X_val) < 10:  # Need minimum samples for validation
                raise ValueError(f"Insufficient validation data in window {window_number}. "
                               f"Got {len(X_val)} samples, need at least 10.")

            # Validate on out-of-sample data
            out_sample_performance = self._evaluate_model(model, X_val, y_val)

            # Calculate outperformance metrics
            benchmark_metrics = config.get('benchmark_metrics', {})
            outperformance = self._calculate_window_outperformance(
                out_sample_performance, benchmark_metrics
            )

            return {
                'window_number': window_number,
                'train_period': window['train_period'],
                'validation_period': window['validation_period'],
                'train_samples': len(X_train),
                'validation_samples': len(X_val),
                'in_sample_performance': in_sample_performance,
                'out_sample_performance': out_sample_performance,
                'outperformance': outperformance,
                'feature_names': feature_names,
                'success': True
            }

        except Exception as e:
            logger.error(f"Failed to process window {window_number}: {e}")
            return {
                'window_number': window_number,
                'train_period': window['train_period'],
                'validation_period': window['validation_period'],
                'error': str(e),
                'success': False
            }

    def _calculate_window_outperformance(
        self,
        performance: Dict[str, float],
        benchmark_metrics: Dict[str, float]
    ) -> Dict[str, Any]:
        """Calculate outperformance metrics for a validation window."""
        outperformance = {}

        for metric in ['accuracy', 'precision', 'recall', 'f1_score', 'roc_auc']:
            if metric in performance and metric in benchmark_metrics:
                model_score = performance[metric]
                benchmark_score = benchmark_metrics[metric]
                improvement = model_score - benchmark_score
                improvement_pct = (improvement / benchmark_score) * 100 if benchmark_score > 0 else 0

                outperformance[metric] = {
                    'model_score': model_score,
                    'benchmark_score': benchmark_score,
                    'improvement': improvement,
                    'improvement_pct': improvement_pct,
                    'outperforms': improvement > 0
                }

        # Calculate overall outperformance
        outperforming_metrics = sum(1 for m in outperformance.values() if isinstance(m, dict) and m.get('outperforms', False))
        total_metrics = len(outperformance)

        return {
            'metric_outperformance': outperformance,
            'outperforming_metrics': outperforming_metrics,
            'total_metrics': total_metrics,
            'overall_outperformance_pct': (outperforming_metrics / total_metrics) * 100 if total_metrics > 0 else 0
        }

    def _analyze_walk_forward_results(
        self,
        validation_results: List[Dict[str, Any]],
        config: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Analyze overall walk-forward validation results."""
        successful_windows = [r for r in validation_results if r.get('success', False)]

        if not successful_windows:
            return {'error': 'No successful validation windows'}

        # Extract performance metrics
        oos_performances = [w['out_sample_performance'] for w in successful_windows]
        is_performances = [w['in_sample_performance'] for w in successful_windows]
        outperformance_scores = [w['outperformance']['overall_outperformance_pct'] for w in successful_windows]

        # Calculate aggregate metrics
        def aggregate_metric(performances, metric_name):
            values = [p.get(metric_name, 0) for p in performances if metric_name in p]
            return {
                'mean': float(np.mean(values)) if values else 0,
                'std': float(np.std(values)) if values else 0,
                'min': float(np.min(values)) if values else 0,
                'max': float(np.max(values)) if values else 0,
                'median': float(np.median(values)) if values else 0
            }

        analysis = {
            'total_windows': len(validation_results),
            'successful_windows': len(successful_windows),
            'success_rate': len(successful_windows) / len(validation_results),

            # Out-of-sample performance (most important)
            'oos_performance': {
                'accuracy': aggregate_metric(oos_performances, 'accuracy'),
                'precision': aggregate_metric(oos_performances, 'precision'),
                'recall': aggregate_metric(oos_performances, 'recall'),
                'f1_score': aggregate_metric(oos_performances, 'f1_score'),
                'roc_auc': aggregate_metric(oos_performances, 'roc_auc')
            },

            # In-sample performance (for comparison)
            'is_performance': {
                'accuracy': aggregate_metric(is_performances, 'accuracy'),
                'precision': aggregate_metric(is_performances, 'precision'),
                'recall': aggregate_metric(is_performances, 'recall'),
                'f1_score': aggregate_metric(is_performances, 'f1_score'),
                'roc_auc': aggregate_metric(is_performances, 'roc_auc')
            },

            # Outperformance analysis
            'outperformance': {
                'mean_pct': float(np.mean(outperformance_scores)),
                'std_pct': float(np.std(outperformance_scores)),
                'consistency': sum(1 for s in outperformance_scores if s > 0) / len(outperformance_scores),
                'best_window': int(np.argmax(outperformance_scores)) + 1,
                'worst_window': int(np.argmin(outperformance_scores)) + 1
            },

            # Stability metrics
            'stability': self._calculate_stability_metrics(oos_performances, config)
        }

        # Add trading implications after analysis is defined
        analysis['trading_implications'] = self._calculate_trading_implications(analysis, config)

        return analysis

    def _calculate_stability_metrics(
        self,
        oos_performances: List[Dict[str, float]],
        config: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Calculate stability metrics for walk-forward validation."""
        if not oos_performances:
            return {}

        # Extract F1 scores as primary stability metric
        f1_scores = [p.get('f1_score', 0) for p in oos_performances]

        mean_f1 = np.mean(f1_scores)
        std_f1 = np.std(f1_scores)
        cv_f1 = std_f1 / mean_f1 if mean_f1 > 0 else 0

        # Consistency: percentage of windows with positive performance
        benchmark_f1 = config.get('benchmark_metrics', {}).get('f1_score', 0.51)
        consistent_windows = sum(1 for s in f1_scores if s > benchmark_f1)
        consistency_ratio = consistent_windows / len(f1_scores)

        # Degradation: IS vs OOS performance drop
        is_f1_scores = []  # Would need to be passed in
        degradation = 0  # Placeholder

        # Stability rating
        if cv_f1 < 0.3 and consistency_ratio > 0.8:
            stability_rating = 'excellent'
        elif cv_f1 < 0.5 and consistency_ratio > 0.6:
            stability_rating = 'good'
        elif cv_f1 < 0.8 and consistency_ratio > 0.4:
            stability_rating = 'moderate'
        else:
            stability_rating = 'poor'

        return {
            'coefficient_of_variation': float(cv_f1),
            'consistency_ratio': float(consistency_ratio),
            'mean_f1': float(mean_f1),
            'std_f1': float(std_f1),
            'stability_rating': stability_rating,
            'degradation': float(degradation)
        }

    def _calculate_trading_implications(
        self,
        analysis: Dict[str, Any],
        config: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Calculate trading implications from walk-forward results."""
        oos_f1 = analysis['oos_performance']['f1_score']['mean']
        benchmark_f1 = config.get('benchmark_metrics', {}).get('f1_score', 0.51)

        # Estimate potential profit improvement
        signal_quality_improvement = (oos_f1 - benchmark_f1) / benchmark_f1
        estimated_profit_boost = signal_quality_improvement * 0.8  # Rough estimate

        # Confidence assessment
        stability_rating = analysis['stability']['stability_rating']
        consistency = analysis['outperformance']['consistency']

        if stability_rating == 'excellent' and consistency > 0.8:
            confidence = 'high'
            recommendation = 'Ready for live deployment with confidence'
        elif stability_rating in ['good', 'excellent'] and consistency > 0.6:
            confidence = 'medium'
            recommendation = 'Suitable for live deployment with monitoring'
        elif stability_rating == 'moderate' or consistency > 0.4:
            confidence = 'low'
            recommendation = 'Requires further optimization before live deployment'
        else:
            confidence = 'very_low'
            recommendation = 'Not recommended for live deployment'

        return {
            'estimated_profit_boost_pct': float(estimated_profit_boost),
            'signal_quality_improvement': float(signal_quality_improvement),
            'confidence_level': confidence,
            'recommendation': recommendation,
            'risk_assessment': self._assess_risk_level(analysis)
        }

    def _assess_risk_level(self, analysis: Dict[str, Any]) -> str:
        """Assess risk level based on walk-forward results."""
        cv = analysis['stability'].get('coefficient_of_variation', 1.0)
        consistency = analysis['outperformance'].get('consistency', 0)

        if cv < 0.3 and consistency > 0.8:
            return 'low'
        elif cv < 0.5 and consistency > 0.6:
            return 'medium_low'
        elif cv < 0.8 and consistency > 0.4:
            return 'medium'
        elif cv < 1.0 and consistency > 0.2:
            return 'medium_high'
        else:
            return 'high'

    def _generate_walk_forward_report(
        self,
        validation_results: List[Dict[str, Any]],
        analysis: Dict[str, Any],
        config: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Generate comprehensive walk-forward validation report."""
        successful_windows = [r for r in validation_results if r.get('success', False)]

        report = {
            'title': 'Walk-Forward Validation Report',
            'generated_at': datetime.now().isoformat(),
            'configuration': config,

            'summary': {
                'total_windows': len(validation_results),
                'successful_windows': len(successful_windows),
                'success_rate': analysis.get('success_rate', 0),
                'validation_period': f"{successful_windows[0]['validation_period']['start']} to {successful_windows[-1]['validation_period']['end']}" if successful_windows else 'N/A',
                'stability_rating': analysis.get('stability', {}).get('stability_rating', 'unknown')
            },

            'performance_summary': {
                'out_of_sample': analysis.get('oos_performance', {}),
                'in_sample': analysis.get('is_performance', {}),
                'outperformance': analysis.get('outperformance', {})
            },

            'stability_analysis': analysis.get('stability', {}),
            'trading_implications': analysis.get('trading_implications', {}),

            'detailed_results': validation_results,

            'recommendations': self._generate_validation_recommendations(analysis, config),

            'metadata': {
                'model_type': config.get('model_type', 'unknown'),
                'validation_methodology': 'walk_forward_expanding_window',
                'benchmark_comparison': config.get('benchmark_metrics', {}),
                'risk_warnings': self._generate_risk_warnings(analysis)
            }
        }

        return report

    def _generate_validation_recommendations(
        self,
        analysis: Dict[str, Any],
        config: Dict[str, Any]
    ) -> List[str]:
        """Generate recommendations based on walk-forward validation results."""
        recommendations = []

        stability = analysis.get('stability', {})
        outperformance = analysis.get('outperformance', {})
        trading = analysis.get('trading_implications', {})

        # Stability-based recommendations
        if stability.get('stability_rating') == 'excellent':
            recommendations.append("✅ Model shows excellent stability across validation windows")
        elif stability.get('stability_rating') == 'good':
            recommendations.append("⚠️ Model shows good stability, suitable for cautious deployment")
        elif stability.get('stability_rating') == 'moderate':
            recommendations.append("⚠️ Model shows moderate stability, further testing recommended")
        else:
            recommendations.append("❌ Model shows poor stability, significant improvements needed")

        # Consistency recommendations
        consistency = outperformance.get('consistency', 0)
        if consistency > 0.8:
            recommendations.append("✅ High consistency in outperformance across windows")
        elif consistency > 0.6:
            recommendations.append("⚠️ Moderate consistency, monitor performance closely")
        else:
            recommendations.append("❌ Low consistency indicates potential overfitting")

        # Trading recommendations
        confidence = trading.get('confidence_level', 'unknown')
        if confidence == 'high':
            recommendations.append("🚀 High confidence for live deployment")
        elif confidence == 'medium':
            recommendations.append("📊 Medium confidence, deploy with position sizing limits")
        else:
            recommendations.append("🛑 Low confidence, not recommended for live deployment")

        # Technical recommendations
        cv = stability.get('coefficient_of_variation', 1)
        if cv > 0.8:
            recommendations.append("🔧 Consider using more robust features or regularization")

        return recommendations

    def _generate_risk_warnings(self, analysis: Dict[str, Any]) -> List[str]:
        """Generate risk warnings based on validation results."""
        warnings = []

        stability = analysis.get('stability', {})
        outperformance = analysis.get('outperformance', {})

        if stability.get('coefficient_of_variation', 0) > 1.0:
            warnings.append("High performance variability may lead to inconsistent results")

        if outperformance.get('consistency', 1) < 0.4:
            warnings.append("Low consistency ratio suggests model may not generalize well")

        if stability.get('stability_rating') in ['poor', 'moderate']:
            warnings.append("Model stability concerns - monitor closely in live trading")

        return warnings

    def _add_basic_regime_indicators(self, df: pd.DataFrame) -> pd.DataFrame:
        """Add basic market regime detection indicators (fallback)."""
        df = df.copy()

        # Ensure we have the required technical indicators
        if 'adx' not in df.columns:
            df['adx'] = self._calculate_adx(df)
        if 'volatility_20' not in df.columns:
            df['volatility_20'] = df['close'].pct_change().rolling(window=20).std()
        if 'rsi' not in df.columns:
            # Calculate RSI if not present
            delta = df['close'].diff()
            gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
            rs = gain / loss
            df['rsi'] = 100 - (100 / (1 + rs))

        # Trend strength using ADX
        df['trend_strength'] = df['adx'].apply(lambda x: 'strong' if x > 25 else 'weak' if x < 20 else 'moderate')

        # Trend direction based on price movement
        df['trend_direction'] = df['close'].rolling(window=20).mean().diff().apply(
            lambda x: 'uptrend' if x > 0 else 'downtrend'
        )

        # Volatility regime
        df['volatility_regime'] = df['volatility_20'].apply(
            lambda x: 'high' if x > df['volatility_20'].quantile(0.75) else 'low' if x < df['volatility_20'].quantile(0.25) else 'normal'
        )

        # Momentum regime
        df['momentum_regime'] = df['rsi'].apply(
            lambda x: 'bullish' if x > 70 else 'bearish' if x < 30 else 'neutral'
        )

        # Convert categorical to numeric
        regime_mapping = {
            'strong': 2, 'moderate': 1, 'weak': 0,
            'high': 2, 'normal': 1, 'low': 0,
            'bullish': 2, 'neutral': 1, 'bearish': 0,
            'uptrend': 1, 'downtrend': -1
        }

        df['trend_strength_num'] = df['trend_strength'].map(regime_mapping)
        df['volatility_regime_num'] = df['volatility_regime'].map(regime_mapping)
        df['momentum_regime_num'] = df['momentum_regime'].map(regime_mapping)
        df['trend_direction_num'] = df['trend_direction'].map(regime_mapping)

        return df
        """Add basic market regime detection indicators (fallback)."""
        df = df.copy()

        # Ensure we have the required technical indicators
        if 'adx' not in df.columns:
            df['adx'] = self._calculate_adx(df)
        if 'volatility_20' not in df.columns:
            df['volatility_20'] = df['close'].pct_change().rolling(window=20).std()
        if 'rsi' not in df.columns:
            # Calculate RSI if not present
            delta = df['close'].diff()
            gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
            rs = gain / loss
            df['rsi'] = 100 - (100 / (1 + rs))

        # Trend strength using ADX
        df['trend_strength'] = df['adx'].apply(lambda x: 'strong' if x > 25 else 'weak' if x < 20 else 'moderate')

        # Trend direction based on price movement
        df['trend_direction'] = df['close'].rolling(window=20).mean().diff().apply(
            lambda x: 'uptrend' if x > 0 else 'downtrend'
        )

        # Volatility regime
        df['volatility_regime'] = df['volatility_20'].apply(
            lambda x: 'high' if x > df['volatility_20'].quantile(0.75) else 'low' if x < df['volatility_20'].quantile(0.25) else 'normal'
        )

        # Momentum regime
        df['momentum_regime'] = df['rsi'].apply(
            lambda x: 'bullish' if x > 70 else 'bearish' if x < 30 else 'neutral'
        )

        # Convert categorical to numeric
        regime_mapping = {
            'strong': 2, 'moderate': 1, 'weak': 0,
            'high': 2, 'normal': 1, 'low': 0,
            'bullish': 2, 'neutral': 1, 'bearish': 0,
            'uptrend': 1, 'downtrend': -1
        }

        df['trend_strength_num'] = df['trend_strength'].map(regime_mapping)
        df['volatility_regime_num'] = df['volatility_regime'].map(regime_mapping)
        df['momentum_regime_num'] = df['momentum_regime'].map(regime_mapping)
        df['trend_direction_num'] = df['trend_direction'].map(regime_mapping)

        return df

    async def _save_shap_report(self, model_name: str, shap_report: Dict[str, Any]):
        """Save SHAP interpretability report."""
        try:
            model_dir = self.models_dir / model_name
            model_dir.mkdir(exist_ok=True)

            report_file = model_dir / "shap_report.json"
            with open(report_file, 'w') as f:
                json.dump(shap_report, f, indent=2, default=str)

            logger.info(f"SHAP report saved: {report_file}")
        except Exception as e:
            logger.error(f"Failed to save SHAP report: {e}")

    async def _integrate_alternative_data(self, df: pd.DataFrame, symbol: str) -> pd.DataFrame:
        """
        Integrate alternative data sources into the feature set.

        Args:
            df: DataFrame with OHLCV and technical indicators
            symbol: Trading symbol

        Returns:
            DataFrame with alternative data features added
        """
        try:
            df = df.copy()

            # Initialize alternative data service if needed
            await self.alternative_data_service.initialize()

            # Get date range for alternative data
            start_date = df.index.min().strftime('%Y-%m-%d')
            end_date = df.index.max().strftime('%Y-%m-%d')

            logger.info(f"Fetching alternative data for {symbol} from {start_date} to {end_date}")

            # Fetch sentiment data
            try:
                sentiment_data = await self.alternative_data_service.get_sentiment_data(
                    symbol=symbol,
                    timeframe="1d",
                    limit=len(df)
                )

                if sentiment_data and 'data' in sentiment_data:
                    # Add sentiment features
                    sentiment_df = pd.DataFrame(sentiment_data['data'])
                    if not sentiment_df.empty and 'timestamp' in sentiment_df.columns:
                        sentiment_df['timestamp'] = pd.to_datetime(sentiment_df['timestamp'])
                        sentiment_df = sentiment_df.set_index('timestamp')

                        # Merge sentiment data
                        for col in ['sentiment_score', 'sentiment_volume', 'bullish_ratio']:
                            if col in sentiment_df.columns:
                                df[f'sentiment_{col}'] = sentiment_df[col]

                        logger.info("Sentiment data integrated successfully")
            except Exception as e:
                logger.warning(f"Failed to fetch sentiment data: {e}")

            # Fetch on-chain metrics
            try:
                onchain_data = await self.alternative_data_service.get_onchain_metrics(
                    symbol=symbol,
                    metrics=['active_addresses', 'transaction_count', 'hash_rate']
                )

                if onchain_data and 'data' in onchain_data:
                    onchain_df = pd.DataFrame(onchain_data['data'])
                    if not onchain_df.empty and 'timestamp' in onchain_df.columns:
                        onchain_df['timestamp'] = pd.to_datetime(onchain_df['timestamp'])
                        onchain_df = onchain_df.set_index('timestamp')

                        # Merge on-chain features
                        for col in ['active_addresses', 'transaction_count', 'hash_rate']:
                            if col in onchain_df.columns:
                                df[f'onchain_{col}'] = onchain_df[col]

                        logger.info("On-chain data integrated successfully")
            except Exception as e:
                logger.warning(f"Failed to fetch on-chain data: {e}")

            # Fetch economic indicators
            try:
                economic_data = await self.alternative_data_service.get_economic_indicators(
                    indicators=['interest_rate', 'inflation', 'gdp_growth']
                )

                if economic_data and 'data' in economic_data:
                    economic_df = pd.DataFrame(economic_data['data'])
                    if not economic_df.empty and 'timestamp' in economic_df.columns:
                        economic_df['timestamp'] = pd.to_datetime(economic_df['timestamp'])
                        economic_df = onchain_df.set_index('timestamp')

                        # Merge economic features
                        for col in ['interest_rate', 'inflation', 'gdp_growth']:
                            if col in economic_df.columns:
                                df[f'economic_{col}'] = economic_df[col]

                        logger.info("Economic data integrated successfully")
            except Exception as e:
                logger.warning(f"Failed to fetch economic data: {e}")

            # Fetch Fear & Greed Index
            try:
                fear_greed_data = await self.alternative_data_service.get_fear_greed_index()

                if fear_greed_data and 'data' in fear_greed_data:
                    fear_greed_df = pd.DataFrame(fear_greed_data['data'])
                    if not fear_greed_df.empty and 'timestamp' in fear_greed_df.columns:
                        fear_greed_df['timestamp'] = pd.to_datetime(fear_greed_df['timestamp'])
                        fear_greed_df = fear_greed_df.set_index('timestamp')

                        # Merge Fear & Greed features
                        for col in ['fear_greed_value', 'fear_greed_classification']:
                            if col in fear_greed_df.columns:
                                df[f'fear_greed_{col}'] = fear_greed_df[col]

                        logger.info("Fear & Greed data integrated successfully")
            except Exception as e:
                logger.warning(f"Failed to fetch Fear & Greed data: {e}")

            # Get composite alternative score
            df = self._get_composite_alternative_score(df)

            return df

        except Exception as e:
            logger.warning(f"Failed to integrate alternative data: {e}")
            return df

    def _get_composite_alternative_score(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Calculate composite alternative data score from multiple sources.

        Args:
            df: DataFrame with alternative data features

        Returns:
            DataFrame with composite score added
        """
        try:
            df = df.copy()

            # Define weights for different data sources
            weights = {
                'sentiment': 0.3,
                'onchain': 0.3,
                'economic': 0.2,
                'fear_greed': 0.2
            }

            # Normalize and combine sentiment features
            sentiment_cols = [col for col in df.columns if col.startswith('sentiment_')]
            if sentiment_cols:
                # Normalize sentiment features to 0-1 scale
                sentiment_normalized = df[sentiment_cols].apply(lambda x: (x - x.min()) / (x.max() - x.min()) if x.max() > x.min() else 0.5)
                df['sentiment_composite'] = sentiment_normalized.mean(axis=1)
            else:
                df['sentiment_composite'] = 0.5

            # Normalize and combine on-chain features
            onchain_cols = [col for col in df.columns if col.startswith('onchain_')]
            if onchain_cols:
                onchain_normalized = df[onchain_cols].apply(lambda x: (x - x.min()) / (x.max() - x.min()) if x.max() > x.min() else 0.5)
                df['onchain_composite'] = onchain_normalized.mean(axis=1)
            else:
                df['onchain_composite'] = 0.5

            # Normalize and combine economic features
            economic_cols = [col for col in df.columns if col.startswith('economic_')]
            if economic_cols:
                economic_normalized = df[economic_cols].apply(lambda x: (x - x.min()) / (x.max() - x.min()) if x.max() > x.min() else 0.5)
                df['economic_composite'] = economic_normalized.mean(axis=1)
            else:
                df['economic_composite'] = 0.5

            # Normalize Fear & Greed Index
            fear_greed_cols = [col for col in df.columns if col.startswith('fear_greed_')]
            if fear_greed_cols:
                fear_greed_normalized = df[fear_greed_cols].apply(lambda x: (x - x.min()) / (x.max() - x.min()) if x.max() > x.min() else 0.5)
                df['fear_greed_composite'] = fear_greed_normalized.mean(axis=1)
            else:
                df['fear_greed_composite'] = 0.5

            # Calculate final composite score
            df['alternative_data_composite'] = (
                weights['sentiment'] * df['sentiment_composite'] +
                weights['onchain'] * df['onchain_composite'] +
                weights['economic'] * df['economic_composite'] +
                weights['fear_greed'] * df['fear_greed_composite']
            )

            logger.info("Composite alternative data score calculated")

            return df

        except Exception as e:
            logger.warning(f"Failed to calculate composite alternative score: {e}")
            df['alternative_data_composite'] = 0.5  # Neutral score
            return df
        """
    async def _integrate_alternative_data(self, df: pd.DataFrame, symbol: str) -> pd.DataFrame:
    async def _integrate_alternative_data(self, df: pd.DataFrame, symbol: str) -> pd.DataFrame:
        """
        Integrate alternative data sources into the feature set.
        
        Args:
            df: DataFrame with OHLCV and technical indicators
            symbol: Trading symbol
        
        Returns:            df: DataFrame with OHLCV and technical indicators
            symbol: Trading symbol

        Returns:
            DataFrame with alternative data features added
        """
        try:
            df = df.copy()

            # Initialize alternative data service if needed
            await self.alternative_data_service.initialize()

            # Get date range for alternative data
            start_date = df.index.min().strftime('%Y-%m-%d')
            end_date = df.index.max().strftime('%Y-%m-%d')

            logger.info(f"Fetching alternative data for {symbol} from {start_date} to {end_date}")

            # Fetch sentiment data
            try:
                sentiment_data = await self.alternative_data_service.get_sentiment_data(
                    symbol=symbol,
                    timeframe="1d",
                    limit=len(df)
                )

                if sentiment_data and 'data' in sentiment_data:
                    # Add sentiment features
                    sentiment_df = pd.DataFrame(sentiment_data['data'])
                    if not sentiment_df.empty and 'timestamp' in sentiment_df.columns:
                        sentiment_df['timestamp'] = pd.to_datetime(sentiment_df['timestamp'])
                        sentiment_df = sentiment_df.set_index('timestamp')

                        # Merge sentiment data
                        for col in ['sentiment_score', 'sentiment_volume', 'bullish_ratio']:
                            if col in sentiment_df.columns:
                                df[f'sentiment_{col}'] = sentiment_df[col]

                        logger.info("Sentiment data integrated successfully")
            except Exception as e:
                logger.warning(f"Failed to fetch sentiment data: {e}")

            # Fetch on-chain metrics
            try:
                onchain_data = await self.alternative_data_service.get_onchain_metrics(
                    symbol=symbol,
                    metrics=['active_addresses', 'transaction_count', 'hash_rate']
                )

                if onchain_data and 'data' in onchain_data:
                    onchain_df = pd.DataFrame(onchain_data['data'])
                    if not onchain_df.empty and 'timestamp' in onchain_df.columns:
                        onchain_df['timestamp'] = pd.to_datetime(onchain_df['timestamp'])
                        onchain_df = onchain_df.set_index('timestamp')

                        # Merge on-chain features
                        for col in ['active_addresses', 'transaction_count', 'hash_rate']:
                            if col in onchain_df.columns:
                                df[f'onchain_{col}'] = onchain_df[col]

                        logger.info("On-chain data integrated successfully")
            except Exception as e:
                logger.warning(f"Failed to fetch on-chain data: {e}")

            # Fetch economic indicators
            try:
                economic_data = await self.alternative_data_service.get_economic_indicators(
                    indicators=['interest_rate', 'inflation', 'gdp_growth']
                )

                if economic_data and 'data' in economic_data:
                    economic_df = pd.DataFrame(economic_data['data'])
                    if not economic_df.empty and 'timestamp' in economic_df.columns:
                        economic_df['timestamp'] = pd.to_datetime(economic_df['timestamp'])
                        economic_df = economic_df.set_index('timestamp')

                        # Merge economic features
                        for col in ['interest_rate', 'inflation', 'gdp_growth']:
                            if col in economic_df.columns:
                                df[f'economic_{col}'] = economic_df[col]

                        logger.info("Economic data integrated successfully")
            except Exception as e:
                logger.warning(f"Failed to fetch economic data: {e}")

            # Fetch Fear & Greed Index
            try:
                fear_greed_data = await self.alternative_data_service.get_fear_greed_index()

                if fear_greed_data and 'data' in fear_greed_data:
                    fg_df = pd.DataFrame(fear_greed_data['data'])
                    if not fg_df.empty and 'timestamp' in fg_df.columns:
                        fg_df['timestamp'] = pd.to_datetime(fg_df['timestamp'])
                        fg_df = fg_df.set_index('timestamp')

                        # Add fear & greed features
                        for col in ['fear_greed_value', 'fear_greed_classification']:
                            if col in fg_df.columns:
                                df[f'fear_greed_{col}'] = fg_df[col]

                        logger.info("Fear & Greed Index integrated successfully")
            except Exception as e:
                logger.warning(f"Failed to fetch Fear & Greed Index: {e}")

            # Fill missing alternative data with forward/backward fill
            alternative_cols = [col for col in df.columns if any(prefix in col for prefix in
                                                                ['sentiment_', 'onchain_', 'economic_', 'fear_greed_'])]
            if alternative_cols:
                df[alternative_cols] = df[alternative_cols].fillna(method='ffill').fillna(method='bfill').fillna(0)

            logger.info(f"Alternative data integration completed. Added {len(alternative_cols)} features.")

            return df

        except Exception as e:
            logger.error(f"Failed to integrate alternative data: {e}")
            return df  # Return original dataframe if integration fails

    async def _get_composite_alternative_score(self, symbol: str) -> Dict[str, Any]:
        """
        Get composite alternative data score for a symbol.

        Args:
            symbol: Trading symbol

        Returns:
            Composite score combining all alternative data sources
        """
        try:
            await self.alternative_data_service.initialize()
            composite_data = await self.alternative_data_service.get_composite_score(symbol)

            if composite_data and 'composite_score' in composite_data:
                return {
                    'composite_score': composite_data['composite_score'],
                    'sentiment_contribution': composite_data.get('sentiment_contribution', 0),
                    'onchain_contribution': composite_data.get('onchain_contribution', 0),
                    'economic_contribution': composite_data.get('economic_contribution', 0),
                    'fear_greed_contribution': composite_data.get('fear_greed_contribution', 0),
                    'market_regime': composite_data.get('market_regime', 'unknown')
                }
            else:
                return {"error": "Failed to retrieve composite score"}

        except Exception as e:
            logger.error(f"Failed to get composite alternative score: {e}")
            return {"error": str(e)}

    async def explain_prediction(self, model_name: str, features: Dict[str, float]) -> Dict[str, Any]:
        """
        Explain a prediction using SHAP.

        Args:
            model_name: Name of the trained model
            features: Feature values for prediction

        Returns:
            SHAP explanation
        """
        try:
            # Load model and metadata
            model, metadata = await self._load_model(model_name)

            # Initialize SHAP interpreter for this model
            success = self.shap_interpreter.load_model(
                str(self.models_dir / f"{model_name}.pkl"),
                metadata.features
            )

            if not success:
                return {"error": "Failed to load model for SHAP interpretation"}

            # Set background data (use a small sample for efficiency)
            # In production, this should be pre-computed during training
            background_size = min(50, len(metadata.features))
            background_data = np.random.randn(background_size, len(metadata.features))
            self.shap_interpreter.set_background_data(background_data)

            # Explain prediction
            explanation = self.shap_interpreter.explain_trading_decision(features, 0.0)

            return explanation

        except Exception as e:
            logger.error(f"Failed to explain prediction: {e}")
            return {"error": str(e)}

    async def get_model_interpretability(self, model_name: str) -> Dict[str, Any]:
        """
        Get interpretability report for a model.

        Args:
            model_name: Name of the model

        Returns:
            Interpretability report
        """        """
        try:
            # Try to load saved SHAP report
            report_file = self.models_dir / model_name / "shap_report.json"
            if report_file.exists():
                with open(report_file, 'r') as f:
                    report = json.load(f)
                return report

            # If no saved report, return basic info
            model, metadata = await self._load_model(model_name)
            return {
                "model_name": model_name,
                "features": metadata.features,
                "performance": metadata.performance,
                "shap_available": False,
                "message": "SHAP report not available. Install shap package and retrain model."
            }

        except Exception as e:
            logger.error(f"Failed to get model interpretability: {e}")
            return {"error": str(e)}


